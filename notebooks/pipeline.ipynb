{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6a68262-e910-4e21-aaaf-1c74febbb0a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports & User-defined variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4824ce9-c211-4e77-9ccb-8da266a8c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545740fa-2eeb-46e3-b7bf-395ec6ee30d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this may take 20-30 seconds\n",
    "import torch\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gemmi\n",
    "import reciprocalspaceship as rs\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from multiprocessing import Pool\n",
    "from itertools import repeat\n",
    "\n",
    "import valdo\n",
    "\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "plt.rc('figure', figsize=(4,2.5))\n",
    "print(\"done with imports.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca04ea52-52d3-4cd6-9cde-984566c97493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncpu=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f6e18-0fb8-41ba-8c01-558585d91b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "bGPU, ncpu=valdo.helper.configure_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f366d283-2a7d-41b2-8a1a-b804e913833e",
   "metadata": {},
   "source": [
    "### Set user-defined variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa7480-58eb-45d6-bf37-79f03c2d3491",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed       = 11231        # random seed for PyTorch and subsampling data (if any) \n",
    "run_prefix        = \"run_20_\" + str(random_seed) + \"_\"\n",
    "\n",
    "# filtering input to MTZ\n",
    "ref_mtz           = '0001'       # use a high-quality dataset as reference\n",
    "mtzs_to_ignore    = []           # MTZs to disregard\n",
    "filter_by_Rfree   = True         # whether to filter out datasets for which refinement was poor (before scaling)\n",
    "max_R_factor      = 0.4          # Worst apo refinement Rfree-factor to keep. otherwise discarded.\n",
    "filter_by_cc      = True         # whether to filter out datasets fwith poor correlation with the reference (after scaling)\n",
    "min_cc_ref        = 0.6          # minimum correlation with the reference\n",
    "\n",
    "# for sorting out indexing ambiguity\n",
    "cc_min_dif        = 0.2          # min correlation difference to determine better reindex operation. otherwise carry through both solutions\n",
    "\n",
    "# VAE (most settings hardcoded below)\n",
    "fraction          = 1.0          # fraction of datasets to work with (default: 1.0)\n",
    "train_fraction    = 0.9          # fraction of working datasets used for training (sensible default: 0.8)\n",
    "latent_dim        = 7            # VAE latent space dimension (sensible default: 7)\n",
    "epochs            = 300          # Number of VAE training epochs (sensible default: 500)\n",
    "include_errors    = True         # whether to use SIGFs in the VAE loss function (sensible default: True)\n",
    "stdof             = 128          # DOF for student t (None -> normal dist); only used when include_errors=True\n",
    "eps               = 0.02         # the smaller eps (>=0), the more strongly estimated errors are taken into account (sensible default: 0.01)\n",
    "\n",
    "# difference maps\n",
    "ml_recon          = True        # Whether to use the ML reconstruction from the VAE rather than a sample (default: False)\n",
    "vae_samples       = 1            # number of samples to draw from the VAE during reconstruction (sensible default: 1), the variation is usually negligible\n",
    "w_pcts            = (95.0,99.99) # percentiles on sigDF and DelDF used in weights calc (sensible default: 95.00, 99.99)\n",
    "low_res_cutoff    = 1000.        # cutoff in A specifying what low-res reflections to keep (prob not helpful! sensible def: 1000.0)\n",
    "\n",
    "# blob finding\n",
    "blob_sig_cutoff   = 3.5          # cutoff for blob search (our default: 3.5)\n",
    "radius_in_A       = 4.0          # radius (in A) of the gaussian smoothing kernel; 3x the sigma (default: 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63960f66-3a1f-445f-8553-931157ad1144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the random number generator for reproducible behavior\n",
    "# change the random seed for another realization of the optimization.\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2010ec6f-a1b5-45d9-a70b-9fad5f2b6ff1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3303685-7006-4327-a960-eee685d21202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use trailing slashes!\n",
    "my_dir             = \"/n/holyscratch01/hekstra_lab/dhekstra/valdo-tests/\" # directory where to keep all the processing steps\n",
    "original_data_path = \"/n/hekstra_lab/people/minhuan/projects/drug/phyllis_backup/pipeline/data/original_data/\" # the MTZs with measured F and sigF\n",
    "\n",
    "# keep the logs and pdb files in the same directory, e.g. with names xxxx.pdb, xxxx.mtz, and xxxx.log\n",
    "refinement_paths=[\"/n/holyscratch01/hekstra_lab/dhekstra/valdo-tests/pipeline_run16/vae/reconstructed/full_recon/refine/short_names/\",\n",
    "                  \"/n/holyscratch01/hekstra_lab/dhekstra/phyllis/PTP1B_DK/pandda_input_models_refined_waters/short/\", \n",
    "                  \"/n/hekstra_lab/people/minhuan/projects/drug/minhuan_backup/pipeline/data/refined/\", # contains refinements for both indexing solutions\n",
    "                  \"/n/holyscratch01/hekstra_lab/dhekstra/valdo-tests/refine/dimple_round_1/\"]\n",
    "\n",
    "refinement_phases = [(\"PH2FOFCWT\",\"PHFOFCWT\"),\\\n",
    "                     (\"PHIF-model\",\"PHFOFCWT\"),\\\n",
    "                     (\"PH2FOFCWT\",\"PHFOFCWT\"),\\\n",
    "                     (\"PHIC_ALL\",\"PHDELWT\")\\\n",
    "                        ]\n",
    "# PHENIX phase options: PHIF-model, PHFC, PH2FOFCWT, PHFOFCWT (0,1)\n",
    "# dimple phase options: PHIC, PHIC_ALL, PHWT, PHIC_ALL_LS (3)\n",
    "\n",
    "def apo_phases_parser_default(file): # for 0,2\n",
    "    return os.path.basename(file)\n",
    "\n",
    "def apo_phases_parser_4digit(file): # for 1\n",
    "    return os.path.basename(file)[0:4]+\".mtz\"\n",
    "    \n",
    "def apo_phases_parser_dimple(file): # for 3\n",
    "    return \"dimple_\"+os.path.basename(file)[0:4]+\"/final.mtz\"\n",
    "\n",
    "\n",
    "apo_phases_parser = apo_phases_parser_dimple\n",
    "\n",
    "which_phases = 3\n",
    "refined_path = refinement_paths[ 2] # keep at 2; used for indexing disambiguation; \n",
    "phasing_path = refinement_paths[which_phases] # used for phasing of difference maps; could be the same as refined_path\n",
    "\n",
    "phase_2FOFC_col_in  = refinement_phases[which_phases][0]\n",
    "phase_FOFC_col_in   = refinement_phases[which_phases][1]\n",
    "phase_2FOFC_col_out = 'refine_PH2FOFCWT'\n",
    "phase_FOFC_col_out  = 'refine_PHFOFCWT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ccfb47-528c-4fa1-b63a-0c23bfcb9376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general pattern for standardizing mtz filenames\n",
    "mtz_file_pattern = r\".*(\\d{4}).*.mtz\"\n",
    "\n",
    "basepath = my_dir + 'pipeline/'\n",
    "data_path      = basepath + 'data/'\n",
    "input_mtz_path = basepath + 'data/mtzs_origin/'\n",
    "reindexed_path = basepath + 'data/mtzs_reindex/'\n",
    "# refined_path   = basepath + 'data/refined/'\n",
    "scaled_path    = basepath + 'data/mtzs_scaled/'\n",
    "\n",
    "# These currently do not carry prefixes bc they will mostly be constant:\n",
    "intersection_path = scaled_path + 'intersection.pkl'\n",
    "union_path        = scaled_path + 'union.pkl'\n",
    "sigF_path         = scaled_path + 'sigF.pkl'\n",
    "\n",
    "vae_path = basepath + 'vae/'\n",
    "vae_reconstructed_path             = vae_path + 'reconstructed/'\n",
    "vae_reconstructed_with_phases_path = vae_path + 'reconstructed_w_phases/'\n",
    "blob_path                          = vae_path + 'blobs/'\n",
    "bound_models_standardized_path= basepath + 'data/bound_models_reindexed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f812a13a-8f28-4be4-b0ec-c6cdd0c46525",
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitude_col        = 'F-obs'\n",
    "amplitude_scaled_col = 'F-obs-scaled'\n",
    "error_col            = 'SIGF-obs'\n",
    "error_scaled_col     = 'SIGF-obs-scaled'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985df0ab-6b83-4f17-b4a1-a1fcdc1c3def",
   "metadata": {},
   "source": [
    "If we want to compare to previously identified bound samples, we'll need this in Step 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f2abc0-66fd-49bb-afed-aa6064b2fc1e",
   "metadata": {},
   "source": [
    "#### Configure paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde0b36a-5565-4a48-860f-9752aa10b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make paths\n",
    "dir_hierarchy=[basepath, data_path, input_mtz_path, reindexed_path, refined_path, scaled_path, vae_path, vae_reconstructed_path, \\\n",
    "               vae_reconstructed_with_phases_path, blob_path, bound_models_standardized_path]\n",
    "for folder in dir_hierarchy:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "        print(f\"{folder:<80}\" + f\"{'created': >20}\")\n",
    "    else:\n",
    "        print(f\"{folder:<80}\" + f\"{'already exists': >20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e101699-3d69-4bca-9033-1736845da4f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Step 1: Diffraction Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabe69ce-7272-4802-993c-293a08402662",
   "metadata": {},
   "source": [
    "The first step involves acquiring diffraction datasets in the `mtz` format. These datasets should follow a specific naming convention, where each file is named with a number followed by the `.mtz` extension (e.g., `0001.mtz`, `0002.mtz`, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb296c84-c04a-460f-b2fe-182c200a10e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03092f4d-aac8-40f3-87e2-d9c0ced26fc3",
   "metadata": {},
   "source": [
    "1. Ensure that you have collected diffraction datasets in the `mtz` format.\n",
    "\n",
    "2. Organize the datasets with sequential numerical names (e.g., `0001.mtz`, `0002.mtz`, etc.). You can do so using the following cell.\n",
    "\n",
    "3. If you have bound-state models already and you want to benchmark against those in Step 9 below, also run the cell `Application to bound state models (for Step 9)`. \n",
    "\n",
    "Following this naming convention will allow datasets to be ready for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4333b1cb-c634-41ba-8cc7-36e5dac8d2b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Copy and Renaming Code\n",
    "_Only need to run the following cell once, can skip on rerun._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a176fe-9487-4818-9a35-d204fe629da1",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following cell is a template for renaming files to the correct naming convention. Change `source_path`, `destination_path`, and extensions as necessary.\n",
    "\n",
    "e.g. `source_path/PTP1B-y0798_mrflagsref_idxs.mtz` -> `destination_path/0798.mtz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2574860-1cb6-4935-b0d9-1433a20e8852",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new_mtzs=valdo.helper.standardize_input_mtzs(source_path=original_data_path, \n",
    "                       destination_path=input_mtz_path, \n",
    "                       mtz_file_pattern=mtz_file_pattern, \n",
    "                       ncpu=ncpu)\n",
    "print(\"\\nCreated \" + str(len(new_mtzs)) + \" standardized MTZ files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd88a92-9666-43e3-a1dc-e685bba103a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Step 2: Reindexing, Automatic Refinement & Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039e036e-c6c0-4283-a5f6-eef58a8dd678",
   "metadata": {},
   "source": [
    "This step focuses on reindexing, automatic refinement and scaling a list of input MTZ files to a reference MTZ file using gemmi. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0945eb2d-c9a7-481a-b162-712d1cafbd09",
   "metadata": {},
   "source": [
    "**Reindexing:** The datasets provided may include samples indexed using different settings to describe the same crystal lattice. To ensure comparability, we reindex each sample to a common indexing scheme by applying reindexing operators. To set the common indexing scheme, we compare each dataset to a common reference dataset. This can be one of the better datasets in the screen. In certain datasets, we may observe minimal disparity in correlation coefficients across various reindexing operations. These instances will be identified and subsequently, we will employ the R-factor from the subsequent autorefinement as an additional criterion.\n",
    "\n",
    "**Automatic Refinement:** After reindexing datasets (ambiguous datasets will be duplicated using all available reindexing operations), we subject them to automatic refinement against the **apo model**. This serves two distinct purposes: firstly, we utilize refinement R-factors to screen out problematic datasets for downstream tasks; secondly, we will employ the phases obtained from the apo refinement in step 7.\n",
    "\n",
    "**Scaling:** The samples are scaled to a reference dataset using a global anisotropic scaling factor by an analytical scaling method that determines the Debye-Waller Factor. The scaling process ensures that structure factor amplitudes are comparable across different datasets, accounting for variabilities such as differences in lattice orientations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8bb40e-7255-4066-9b08-a21e293c2211",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08a5d0-92aa-4f96-8e03-2476910481b6",
   "metadata": {},
   "source": [
    "1. Import the required library, `valdo`.\n",
    "\n",
    "2. Call the `reindex_files()` function from `valdo.reindex`. The `reindex_files()` function will enumerate possible reindexing operations for any space group and apply them to each input MTZ file. It will select the operation with the highest correlation with the reference dataset. The reindexed files will be saved in the specified output folder, following `####_i.mtz` naming convention: `####` is the same file idx as before and `i` is the id of the best reindex operations, `0` means identical operation. \n",
    "\n",
    "    This function can be called with the following parameters:\n",
    "    - `input_files`: List of paths to input MTZ files to be reindexed.\n",
    "    - `reference_file`: Path to the reference MTZ file.\n",
    "    - `output_folder`: Path to the folder where the reindexed MTZ files will be saved.\n",
    "    - `columns`: A list containing the names of the columns in the dataset that represent the amplitude and the error column.\n",
    "\n",
    "  \n",
    "3. You have the option to perform automatic refinement using either PHENIX or your preferred refinement software, such as dimple. In valdo, we provide a **command-line tool** specifically designed for automatic refinement using PHENIX. Our tests have shown that initiating the process with a lone apo model produces adequate phases and models for subsequent real-space maps in step 7. An example configuration file named `refine_drug.eff` can be located in the `notebook/` directory.\n",
    "\n",
    "    *Code Example:* `valdo.refine --pdbpath \"xxx/xxx_apo.pdb\" --mtzpath \"xxx/*.mtz\" --output \"yyy/\" --eff \"xxx/refine_drug.eff\"`\n",
    "\n",
    "    > ***Note***: The refinement process should be executed in a command-line interface separate from the notebook. Keep in mind that the duration of this process depends on the quantity of datasets and the refinement software being employed. With PHENIX, each dataset typically takes approximately ~3 minutes. Therefore, it is advisable to run the aforementioned refinement concurrently on a distributed computing cluster for efficiency.\n",
    "\n",
    "   Once the refinement is done, results can be found in folder `yyy/` as you set. Each dataset will have at least 4 files:\n",
    "\n",
    "   - `refine_####_001.pdb`: Refined model\n",
    "   - `refine_####_001.mtz`: Refined mtz file with phases\n",
    "   - `refine_####_001.log`: Log file during the refinement, containing the R-factors before and after refinement\n",
    "   - `refine_####_001.eff`: Complete configuration file of the refinement \n",
    "\n",
    "   Generate a list of problematic datasets based on the R-factors, screen out for downstreaming tasks.\n",
    "\n",
    "5. Create a `Scaler` object by providing the path to the reference MTZ file.\n",
    "\n",
    "6. Call the `batch_scaling()` method of the `Scaler` object. The `batch_scaling()` method will apply the scaling process to each input MTZ file and save the scaled MTZ files in the specified output folder. Scaling metrics, such as least squares values and correlations, will be saved in the report file.\n",
    "\n",
    "    This function can be called with the following parameters:\n",
    "    - `mtz_path_list`: List of paths to input MTZ files to be scaled.\n",
    "    - `outputmtz_path`: Path to the folder where the scaled MTZ files will be saved (optional, default is `./scaled_mtzs/`).\n",
    "    - `reportfile`: Path to the file where scaling metrics will be saved (optional, default is `./scaling_data.json`).\n",
    "    - `verbose`: Whether to display verbose information during scaling (optional, default is `True`).\n",
    "    - `n_iter`: Number of iterations for the analytical scaling method (optional, default is `5`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f13e11-7b4d-4845-84b0-b59451f6f3db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Reindexing Code\n",
    "_Only need to run the following once, so can skip on rerun._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de3e185-789c-4768-adfd-25632d3baeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of files to be reindexed\n",
    "file_list = glob.glob(input_mtz_path + \"*mtz\")\n",
    "file_list.sort()\n",
    "\n",
    "# Drop mtzs in the ignore list\n",
    "for mtz in mtzs_to_ignore:\n",
    "    file_list.remove(input_mtz_path+mtz)\n",
    "print(\"Working with \" + str(len(file_list)) + \" MTZ files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4147fd29-496d-4bdb-90c1-f0bf8e2de298",
   "metadata": {},
   "source": [
    "Select a good dataset as your reference (see the variable `ref_mtz` above).\n",
    "\n",
    "**Critical**: The phases we will use below to calculate difference maps must come from models that have been indexed _consistent_ with the indexing convention chosen here (by the choice of our reference dataset). Otherwise, the difference maps will be meaningless!! An easy way to assure consistency is by obtaining the phases from refinement against the reindexed data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a1cab5-8e5f-46b9-92b5-99a77cff5f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# Reindexes a list of input MTZ files to a reference MTZ file using gemmi, took ~1 min\n",
    "reference_idx = file_list.index(input_mtz_path+ref_mtz+\".mtz\")\n",
    "\n",
    "if ncpu >1:\n",
    "    reindexing_record=valdo.reindex.reindex_files_pool(\n",
    "        input_files=file_list, \n",
    "        reference_file=file_list[reference_idx], \n",
    "        output_folder=reindexed_path,\n",
    "        columns=[amplitude_col, error_col],\n",
    "        wcorr=True, \n",
    "        ncpu=ncpu,\n",
    "        cc_min_dif=cc_min_dif\n",
    ")\n",
    "else:\n",
    "    reindexing_record=valdo.reindex.reindex_files(\n",
    "        input_files=file_list, \n",
    "        reference_file=file_list[reference_idx], \n",
    "        output_folder=reindexed_path,\n",
    "        columns=[amplitude_col, error_col],\n",
    "        wcorr=True, \n",
    "        cc_min_dif=cc_min_dif\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be47577c-22d9-4f14-a63d-d88bcd366229",
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexing_record.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ef12b-ec9c-4bc0-8bc8-a2a5c07073dc",
   "metadata": {},
   "source": [
    "If reindexing is relevant, it is good to check the gap in correlation coefficients with the reference across indexing solutions. In the case of PTP-1B, any gap < 0.2 seems suspicious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df9d95c-b960-4d26-959d-26461380d524",
   "metadata": {},
   "outputs": [],
   "source": [
    "CC_dif=reindexing_record[\"CC_symop1\"] - reindexing_record[\"CC_symop0\"]\n",
    "CC_max=reindexing_record.apply(lambda x: max(x[\"CC_symop1\"], x[\"CC_symop0\"]), axis=1)\n",
    "plt.grid()\n",
    "plt.hist(CC_dif,100)\n",
    "plt.xlabel(\"CC (symop 1) - CC (symop 0)\")\n",
    "plt.ylabel(\"count per bin\")\n",
    "plt.show()\n",
    "\n",
    "plt.grid()\n",
    "plt.plot(CC_max, CC_dif,'.')\n",
    "plt.ylabel(\"CC (symop 1) - CC (symop 0)\")\n",
    "plt.xlabel(\"max CC over symops\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cd9252-fc67-429b-be6f-710985e31bd7",
   "metadata": {},
   "source": [
    "Ambiguous datasets with more than 1 duplicates are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc6350e-4ad4-4143-a211-b5e69e4d4b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexing_record[reindexing_record[\"num_duplicates\"] > 1].head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996622f0-1b8e-4ead-ae4e-0e14b8068454",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Automatic Refinement Code Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5ae585-1633-465f-89ee-9ab468ec9030",
   "metadata": {},
   "source": [
    "The automatic refinement should happen outside the notebook with the reindexed mtz files. If no reindex is needed, you should use the input mtz files instead.\n",
    "\n",
    "**Note:** It is recommended to run the automatic refinement in a parallel manner for many datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d2dda4-8c68-4b62-8231-a79d875303d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example command to run automatic refinement:\")\n",
    "print(\" \")\n",
    "if reindexing_record is not None:\n",
    "    print(f\"valdo.refine --pdbpath 'xxx/xxx_apo.pdb' --mtzpath '{reindexed_path}*.mtz' --output '{refined_path}' --eff 'xxx/refine_drug.eff'\\n\")\n",
    "else:\n",
    "    print(f\"valdo.refine --pdbpath 'xxx/xxx_apo.pdb' --mtzpath '{input_mtz_path}*.mtz' --output '{refined_path}' --eff 'xxx/refine_drug.eff'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af78630-7034-4985-9095-0f7dbfa07a40",
   "metadata": {},
   "source": [
    "### After Automatic refinement\n",
    "After completing the refinement process, all outcomes will be located in the `refined_path`. If you are utilizing PHENIX for refinement, you can generate refinement statistics using the provided codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e15ee6e-7bdf-4cf8-a27c-451dbcc660cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loglist = glob.glob(os.path.join(refined_path, \"*log\"))\n",
    "loglist.sort()\n",
    "\n",
    "idx = []\n",
    "symop = []\n",
    "Rw_start = []\n",
    "Rf_start = []\n",
    "Rw_final = []\n",
    "Rf_final = []\n",
    "time = []\n",
    "\n",
    "print(os.path.basename(loglist[0]).replace('.','_').split(\"_\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2d259-70ca-4f86-ac9d-188f0f65d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(loglist):\n",
    "    with open(file, 'r') as f:\n",
    "        log = f.readlines()\n",
    "        # print(os.path.basename(file).split(\"_\")) # may need to adjust next line accordingly\n",
    "    # _, idx_temp, symop_temp, _, _, _  = os.path.basename(file).split(\"_\")\n",
    "    _, idx_temp, symop_temp,_,_  = os.path.basename(file).replace('.','_').split(\"_\")\n",
    "    try:\n",
    "        Rw_final_temp, Rf_final_temp = float(log[-2].replace(\",\",\" \").split(\" \")[3]), float(log[-2].replace(\",\",\" \").split(\" \")[-1]) # Final\n",
    "        Rw_start_temp, Rf_start_temp = float(log[-3].replace(\",\",\" \").split(\" \")[3]), float(log[-3].replace(\",\",\" \").split(\" \")[-1]) # Start\n",
    "        time_temp = float(log[-1].strip().split(\" \")[-2]) # Time\n",
    "    except Exception as e:\n",
    "        print(\"Sample\", idx_temp+\"_\"+symop_temp, \"refinement fails!\")\n",
    "        print(log[-2])\n",
    "        print(log[-3])\n",
    "        print(log[-1])\n",
    "        print(e)\n",
    "        Rw_final_temp, Rf_final_temp = float(\"nan\"), float(\"nan\")\n",
    "        Rw_start_temp, Rf_start_temp = float(\"nan\"), float(\"nan\")\n",
    "        time_temp = float(\"nan\")\n",
    "    idx.append(idx_temp)\n",
    "    symop.append(symop_temp)\n",
    "    Rw_start.append(Rw_start_temp)\n",
    "    Rf_start.append(Rf_start_temp)\n",
    "    Rw_final.append(Rw_final_temp)\n",
    "    Rf_final.append(Rf_final_temp)\n",
    "    time.append(time_temp)\n",
    "\n",
    "df_refine = pd.DataFrame({\n",
    "    \"file_idx\"   : idx,\n",
    "    \"symop\"      : symop,\n",
    "    \"Rw_start\"   : Rw_start,\n",
    "    \"Rf_start\"   : Rf_start,\n",
    "    \"Rw_final\"   : Rw_final,\n",
    "    \"Rf_final\"   : Rf_final,\n",
    "    \"time(s)\"    : time\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a78af95-82d9-49d1-b0cc-25229a8d0d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279a1deb-a96d-49cc-9946-5cde098ded1e",
   "metadata": {},
   "source": [
    "#### Using apo refinement R-factors to determine reindexing operations in ambiguous datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c522e-e6c4-48f5-8344-31309ba80258",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    reindexing_record = pd.read_pickle(os.path.join(reindexed_path, \"reindex_record.pkl\"))\n",
    "    reindexing_record[\"file_idx\"]=reindexing_record[\"file_idx\"].astype('string')\n",
    "except:\n",
    "    print(\"Failed to read reindexing record.\")\n",
    "    reindexing_record = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ddc34d-6bec-4b21-81f4-74217d7b3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "ambiguous_idx = reindexing_record[reindexing_record[\"num_duplicates\"]>1][\"file_idx\"].tolist()\n",
    "df_ambiguous = df_refine[df_refine[\"file_idx\"].apply(lambda x: x in ambiguous_idx)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2973fad-313b-48a9-81b8-1a3e1223c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ambiguous_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf478ee-d042-45c0-a75e-c54767077b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_worseops = df_ambiguous.loc[df_ambiguous.groupby(by='file_idx')['Rf_final'].idxmax()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed94a2d0-f217-4ba6-be41-a8e36cb6dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_worseops.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a530da-255b-423e-ac2c-4c473b561072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will also filter out all datasets for which no refinement is available. Otherwise these will linger as polluting duplicates\n",
    "refined=df_refine[\"file_idx\"].to_list()\n",
    "filter_list0=[]\n",
    "for file_idx in reindexing_record[\"file_idx\"].to_list():\n",
    "    if file_idx not in refined:\n",
    "        filter_list0.append(file_idx)\n",
    "print(filter_list0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e832c6-e5a4-410c-8ef0-dac7577f5df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List 1 of files will be filtered out from reindexed mtzs: symops with worse R-factors in duplicates\n",
    "filter_list1 = (df_worseops[\"file_idx\"] + \"_\" + df_worseops[\"symop\"] + '.mtz').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eaad20-b3d0-43e3-aae9-1aa4b93683a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filter_list1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01709cf2-ea4e-4e8b-bdc7-e2ed4fbec996",
   "metadata": {},
   "source": [
    "#### Screen out datasets with bad R-factors in apo refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4568856a-8b2c-4b0a-af9a-5c8e6bddc079",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_badR = df_refine[df_refine[\"Rf_final\"] > max_R_factor].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb696b0-3e5d-49bc-9afa-33f4b0fe8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_list2=[]\n",
    "if filter_by_Rfree:\n",
    "    filter_list2 = (df_badR[\"file_idx\"] + '.mtz').tolist()\n",
    "    print(filter_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7dbe717-727c-45e8-9b82-6fd524e11973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get union of filter_list1 and filter_list2\n",
    "filter_list = list(set().union(filter_list0, filter_list1, filter_list2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d117a7e-357a-4008-aed9-5e2fa2129726",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scaling Code\n",
    "_Only need to run the following cell once, can skip on rerun._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04a8177-b4e4-42d9-a194-83b85858a330",
   "metadata": {},
   "outputs": [],
   "source": [
    "reindexing_record.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0d95fe-1efa-4f7d-81b8-aeed41c38372",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reindexing_record is None:\n",
    "    file_list = glob.glob(input_mtz_path + \"*mtz\")\n",
    "    reference_idx = file_list.index(input_mtz_path+ref_mtz+\".mtz\") \n",
    "else:\n",
    "    file_list = glob.glob(reindexed_path + \"*mtz\")\n",
    "    idx=reindexing_record[reindexing_record.file_idx==ref_mtz].iloc[0]\n",
    "    reindexed_ref_mtz=ref_mtz+\"_\"+str(idx[\"best_symop\"])+\".mtz\"\n",
    "    reference_idx = file_list.index(reindexed_path + reindexed_ref_mtz) \n",
    "print(\"Our reference will be \" + file_list[reference_idx])\n",
    "file_list.sort()\n",
    "\n",
    "n=0\n",
    "for mtz in filter_list:\n",
    "    try:\n",
    "        file_list.remove(reindexed_path+mtz)\n",
    "    except:\n",
    "        n=n+1\n",
    "print(f\"{n} MTZ files were alread not in the list and did not need to be removed.\")\n",
    "    \n",
    "print(\"We have \" + str(len(file_list)) + \" MTZ files ready for scaling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a967005f-b041-4628-82c0-edaa46e47a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaled_path)\n",
    "# print(glob.glob(scaled_path + \"*mtz\"))\n",
    "prior_scaled_mtzs=glob.glob(scaled_path + \"*mtz\")\n",
    "print(f\"contained \" +str(len(prior_scaled_mtzs)) + \" files.\")\n",
    "print(\"Removing...\")\n",
    "for f in prior_scaled_mtzs:\n",
    "    os.remove(f)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c7e28a-b2f4-4f08-bf6d-25d6276f91f0",
   "metadata": {},
   "source": [
    "The following optimization of scales sometimes generates numerical warnings about overflows. Let us know if this happens a lot. Without parallelization, this takes about 0.25s/dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d989673-1405-4abe-b168-7fbdda97c406",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Scales all datasets to the previously provided reference, writes a `metrics.pkl`, take ~30s\n",
    "# Returns some metrics on the quality of the dataset.\n",
    "\n",
    "if ncpu > 1:\n",
    "    scaler = valdo.Scaler_pool(reference_mtz=file_list[reference_idx],columns=[amplitude_col, error_col],verbose=False, n_iter=15,ncpu=ncpu)\n",
    "    scaling_metrics = scaler.batch_scaling(mtz_path_list=file_list, \n",
    "                                   outputmtz_path=scaled_path,\n",
    "                                   prefix=run_prefix)\n",
    "else:\n",
    "    scaler = valdo.Scaler(reference_mtz=file_list[reference_idx])\n",
    "    scaling_metrics = scaler.batch_scaling(mtz_path_list=file_list, \n",
    "                                   outputmtz_path=scaled_path,\n",
    "                                   prefix=run_prefix, \n",
    "                                   verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8d1f2f-b9ba-463a-8811-2015cbe59794",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_metrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161d6643-f735-4eb1-9f63-382ffa3e8912",
   "metadata": {},
   "source": [
    "- Most of the datasets for which the LS goes up also have bad Rfree.\n",
    "- Keep in mind that LS scales with number of reflections. A better comparison would normalize for that.\n",
    "- There are about 40 datasets that were not refined and lack an Rfree. I don't know why. Based on the table above, these were not necessarily the worst datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff5d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scaling_metrics[\"start_LS\"].to_numpy(),scaling_metrics[\"end_LS\"].to_numpy(),'.',alpha=0.25)\n",
    "xlim=plt.xlim()\n",
    "plt.plot(xlim,xlim,'r-')\n",
    "plt.xlabel(\"Starting LS\")\n",
    "plt.ylabel(\"Final LS\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c24816-8832-4a38-953e-e0e02df2b4ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Step 3: Normalization & Preparation of VAE input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4c5cf6-11a1-45f1-bc60-cb64aa6b7b06",
   "metadata": {},
   "source": [
    "This step involves normalizing the scaled structure factor amplitudes obtained in the previous step. The input is restricted to only those Miller indices present in the _intersection_ of all datasets, and the VAE will predicts structure factor amplitudes for all Miller indices in the _union_ of all datasets.\n",
    "\n",
    "Additionally, we standardize all the input data, such that the structure factor amplitudes for each Miller index in the union of all datasets have a mean of zero and a unit variance across datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e23f578-3067-43a8-a3b8-c78fbcdb4399",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9769802-6333-4d7b-adab-37a54da31847",
   "metadata": {},
   "source": [
    "1. Import the required library, `valdo.preprocessing`.\n",
    "\n",
    "2. Find the intersection and union of the scaled datasets using the following functions:\n",
    "\n",
    "   - `find_intersection()`: Finds the intersection of `amplitude_col` from multiple input MTZ files and saves the result to the specified output pickle file. Arguments include the following:\n",
    "\n",
    "      - `input_files`: List of input MTZ file paths.\n",
    "      - `output_path`: Path to save the output pickle file containing the intersection data.\n",
    "      - `amplitude_col`: Name of the column in the dataset that represents the scaled amplitude (default is 'F-obs-scaled').\n",
    "\n",
    "   - `find_union()`: Finds the union of `amplitude_col` from multiple input MTZ files and saves the result to the specified output pickle file. Arguments are the same as `find_intersection()`.\n",
    "\n",
    "3. Generate the VAE input and output data using the `generate_vae_io()` function. This standardizes the intersection dataset using mean and standard deviation calculated from the union dataset. The standardized intersection becomes the VAE input, while the standardized union becomes the VAE output. Both the VAE input and output are saved to the specified folder. \n",
    "\n",
    "    This function can be called with the following parameters:\n",
    "\n",
    "    - `intersection_path`: Path to the intersection dataset pickle file.\n",
    "    - `union_path`: Path to the union dataset pickle file.\n",
    "    - `io_path`: Path to the output folder where the VAE input and output will be saved. Mean and standard deviation data calculated from the union dataset will also be saved in this folder as `union_mean.pkl` and `union_sd.pkl`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deafcf0-7551-4c81-b9cc-dfdc7f66c3dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Filtering Samples Code\n",
    "\n",
    "_Only need to run this code once, can skip on rerun._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bdb137-e08c-48bd-8166-a577f5b12c05",
   "metadata": {},
   "source": [
    "In this example, we remove samples with low `end_corr`. This ensures that our VAE is trained with high quality samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dbb815-398f-4797-a191-ea9ed6a8bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all scaled files to use as input and output for the VAE\n",
    "\n",
    "file_list = glob.glob(scaled_path + \"*mtz\")\n",
    "file_list.sort()\n",
    "print(\"Current file list contains \" + str(len(file_list)) + \" entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ab1fd-9e96-4dcd-9572-010c5022df9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "This following cell removes samples with `end_corr < min_cc_ref` or if `end_corr = NA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ad07d-e684-4aed-b568-9117ab062cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_pickle(scaled_path + run_prefix + \"scaling_metrics.pkl\")\n",
    "\n",
    "if filter_by_cc:\n",
    "    low_corr_files = list(metrics_df[ (metrics_df['end_corr'] < min_cc_ref) | \\\n",
    "                                      (metrics_df['end_corr'].isnull())]['file'])\n",
    "    low_corr_files = [scaled_path + x + '.mtz' for x in low_corr_files]\n",
    "    file_list = [file for file in file_list if file not in low_corr_files]\n",
    "\n",
    "with open(os.path.join(vae_path, 'filtered_file_list.pkl'), 'wb') as f:\n",
    "    pickle.dump(file_list, f)\n",
    "print(\"The file list contains \" + str(len(file_list)) + \" entries after filtering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412569ad-4787-4f7d-a2a1-4af64e49e999",
   "metadata": {},
   "source": [
    "##### For testing purposes, we will subsample the list of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2cb462-46ef-43b1-b7a1-fe2df24780e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fraction < 1.0:\n",
    "    file_list = sample(file_list, int(fraction * len(file_list)))\n",
    "print(\"Subsampling \" + str(len(file_list)) + \" entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbb01bb-9db7-476b-93f8-e943cb503edd",
   "metadata": {},
   "source": [
    "### Generating VAE Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94831fcd-f842-4c37-96ec-3e750a245b45",
   "metadata": {},
   "source": [
    "The following cells generate the VAE input and output. The first cell takes about 3 min for PTP1B; the second one about 10 min (both on a CPU).\n",
    "\n",
    "**Together, these two steps are the slowest part of the pipeline (when using multiple cpu). Please be patient!**\n",
    "If you are not changing anything about scaling or dataset filtering, you can skip them next time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64475a6-ae79-4136-81da-d955fe548e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Creates an `intersection.pkl` file at the specified path\n",
    "# This is the intersection of all the scaled files provided\n",
    "valdo.preprocessing.find_intersection(input_files=file_list, \n",
    "                  output_path=intersection_path,\n",
    "                  amplitude_col=amplitude_scaled_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ec5d5a-d74f-4730-ad5c-8a25b4823fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Creates a `union.mtz` file at the specified path\n",
    "# This is the union of all the scaled files provided\n",
    "# This is a lot slower than the intersection step about. The step is \n",
    "#   about linear in the number of files, 0.5 sec/file on my CPU.\n",
    "# Inclusion of errors does not affect speed -- *always include them*\n",
    "\n",
    "valdo.preprocessing.find_union(input_files=file_list, \n",
    "                               output_path=union_path,\n",
    "                               sigF_path=sigF_path,\n",
    "                               amplitude_col=amplitude_scaled_col, \n",
    "                               include_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f2c8d-3d30-414b-9c68-c194dbc9b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Generates VAE input and output data from the intersection and union datasets\n",
    "# Inclusion of errors does not affect speed => always include them here.\n",
    "# Note that we do not currently prefix the VAE input\n",
    "valdo.preprocessing.generate_vae_io(intersection_path=intersection_path, \n",
    "                                    union_path=union_path, \n",
    "                                    sigF_path=sigF_path,\n",
    "                                    io_folder=vae_path, \n",
    "                                    prefix=\"\", \n",
    "                                    include_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20af9fe-70d3-4683-b199-c4ee0d8b946f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Step 4: VAE Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bbe4f6-6158-403c-855e-a9ae78a9a00b",
   "metadata": {},
   "source": [
    "In this step, we train the VAE model using the provided VAE class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c3057-4c5c-43fd-94bc-fc718affe32e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a414ee3a-0bc9-4344-8ab4-1090ac02eb68",
   "metadata": {},
   "source": [
    "1. Load the VAE input and output data that was generated in the previous step.\n",
    "\n",
    "2. Initialize the VAE model with the desired hyperparameters. Tune-able hyperparameters include the following:\n",
    "    - `n_dim_latent`: Number of dimensionality in latent space (optional, default `1`)\n",
    "\n",
    "    - `n_hidden_layers`: Number of hidden layers in the encoder and decoder. If an int is given, it will applied to both encoder and decoder; If a length 2 list is given, first int will be used for encoder, the second will be used for decoder\n",
    "\n",
    "    - `n_hidden_size`: Number of units in hidden layers. If an int is given, it will be applied to all hidden layers in both encoder and decoder; otherwise, an array with length equal to the number of hidden layers can be given, the number of units will be assigned accordingly.\n",
    "\n",
    "    - `activation` : Activation function for the hidden layers (optional, default `tanh`)\n",
    "\n",
    "3. Split the data into training and validation sets. Randomly select a subset of indices for training and use the rest for validation.\n",
    "\n",
    "4. Convert the data into PyTorch tensors.\n",
    "\n",
    "5. Set up the optimizer for training.\n",
    "\n",
    "6. Train the VAE model using the `train()` method. The training process involves minimizing the ELBO (Evidence Lower Bound) loss function, which consists of a Negative Log-Likelihood (NLL) term and a Kullback-Leibler (KL) divergence term. Arguments used in this function include:\n",
    "\n",
    "    - `x_train`: Input data for training the VAE, a PyTorch tensor representing the VAE input data. \n",
    "\n",
    "    - `y_train`: Output data for training the VAE, a PyTorch tensor representing the VAE output data. \n",
    "\n",
    "    - `optim`: The optimizer used for training the VAE, a PyTorch optimizer object, such as `torch.optim.Adam`, that specifies the optimization algorithm and its hyperparameters, including the learning rate (`lr`).\n",
    "\n",
    "    - `x_val`: Input data for validation during training. (optional, default is `None`).\n",
    "\n",
    "    - `y_val`: Output data for validation during training. (optional, default is `None`).\n",
    "\n",
    "    - `epochs`: The number of training epochs (epoch: a single pass through the data).\n",
    "\n",
    "    - `batch_size`: The batch size used during training. If an integer is provided, the same batch size will be used for all epochs. If a list of integers is provided, it should have the same length as the number of epochs, and each value in the list will be used as the batch size for the corresponding epoch. Default is `256`.\n",
    "\n",
    "    - `w_kl`: The weight of the Kullback-Leibler (KL) divergence term in the ELBO loss function. The KL divergence term encourages the latent distribution to be close to a prior distribution (usually a standard normal distribution). A higher value of `w_kl` will increase the regularization strength on the latent space. Default is `1.0`.\n",
    "    - `include_errors`, `eps`, `stdof`: when including errors, the loss will be weighted by the errors in the observed amplitudes. A small fixed amount, `eps` can be added to these error estimates to diminish the contribution of the reflections with very small sigF (e.g. around 0.005 after normalization). `stdof` toggles the loss function between Student t (finite `stdof`) and Normal (`stdof=None`).\n",
    "\n",
    "    **Note:** The VAE class internally keeps track of the training loss (`loss_train`) and its components (NLL and KL divergence) during each batch of training. These values can be accessed after training to monitor the training progress and performance. The `loss_train` attribute of the VAE object will be a list containing the training loss values for each batch during training. The `loss_names` attribute contains the names of the loss components: \"Loss\", \"NLL\", and \"KL_div\". These attributes are updated during training and can be used for analysis or visualization.\n",
    "\n",
    "7. Save the trained VAE model for future use (optional)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd901df-c30b-4d73-95a3-79e18491136e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f672b-47ce-464c-997b-3ed9ececd1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VAE I/O Files Generated\n",
    "# note that we do not use a prefix since these files are likely to change less frequently than the runs.\n",
    "vae_input  = np.load(vae_path + 'vae_input.npy')\n",
    "vae_output = np.load(vae_path + 'vae_output.npy')\n",
    "vae_sigF   = np.load(vae_path + 'vae_sigF.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c70c992-6ff2-4890-8330-09384cba57f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "print(vae_input.shape)\n",
    "print(vae_output.shape)\n",
    "print(vae_sigF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d667f-30a3-468d-91ac-c3067d898223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to see the training target in all its glory...\n",
    "# union = pd.read_pickle(union_path)\n",
    "# union.iloc[:5,:20]\n",
    "\n",
    "# or the (normalized) sigF\n",
    "# sigF = pd.read_pickle(sigF_path)\n",
    "# sigF.iloc[:5,:20]\n",
    "\n",
    "# SNR_ratio = sigF.iloc[:500,:500].values.flatten()/union.iloc[:500,:500].values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29715979-2e73-4a58-87d5-d60cf33b2d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(vae_output[10,:],vae_sigF[10,:],'.',alpha=0.01)\n",
    "plt.xlabel(\"Normalized input amplitudes (can be negative!)\")\n",
    "plt.ylabel(\"Normalized SigF\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807935b6-0120-4494-a5fb-02faf1da4a86",
   "metadata": {},
   "source": [
    "#### Setting up the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd945d-04b7-4c46-b320-a236bad7b606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Specify VAE Parameters; takes ~1 sec\n",
    "# Only execute once. Otherwise it may throw an error.Restart kernel if this cell throws an error\n",
    "latent_dimension = latent_dim\n",
    "train_fraction = train_fraction\n",
    "epochs = epochs # default: 300\n",
    "\n",
    "vae = valdo.VAE(n_dim_i = vae_input.shape[1], \n",
    "      n_dim_o = vae_output.shape[1], \n",
    "      n_dim_latent = latent_dimension, \n",
    "      n_hidden_layers = [3, 6], \n",
    "      n_hidden_size = 100, \n",
    "      activation = torch.relu)\n",
    "\n",
    "# Randomly select (train_fraction) indices for training\n",
    "choice = np.random.choice(vae_input.shape[0], int(train_fraction*vae_input.shape[0]), replace=False)    \n",
    "train_ind = np.zeros(vae_input.shape[0], dtype=bool)\n",
    "train_ind[choice] = True\n",
    "test_ind = ~train_ind\n",
    "print(\"Size of training set = \" + str(np.sum(train_ind)))\n",
    "print(\"Size of test set = \" + str(np.sum(test_ind)))\n",
    "\n",
    "# Split the input and output data into training and validation sets\n",
    "x_train, x_val = vae_input[train_ind], vae_input[test_ind]\n",
    "y_train, y_val = vae_output[train_ind], vae_output[test_ind]\n",
    "e_train, e_val = vae_sigF[train_ind], vae_sigF[test_ind] # error estimates for y\n",
    "\n",
    "# Convert the data to torch tensors\n",
    "x_train, x_val, y_train, y_val, e_train, e_val = torch.tensor(x_train), torch.tensor(x_val), \\\n",
    "                                                 torch.tensor(y_train), torch.tensor(y_val), \\\n",
    "                                                 torch.tensor(e_train), torch.tensor(e_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d57a324-ec20-47bd-ba2c-1bc12b6d7766",
   "metadata": {},
   "source": [
    "#### Training the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8754446-c53c-4085-b5ea-5683f3843ad2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Set up the optimizer and train the VAE\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "vae.train(x_train, y_train, e_train, optimizer, x_val, y_val, e_val, epochs=epochs, \n",
    "          batch_size=100, w_kl=1.0,\n",
    "          eps=eps,\n",
    "          include_errors=include_errors,\n",
    "          stdof=stdof, \n",
    "          verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8ce422-7576-4178-9192-be428994be20",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the trained VAE model\n",
    "vae.save(vae_path + run_prefix + 'trained_vae.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb40c5db-0667-4599-a72f-7ee75fbd199c",
   "metadata": {},
   "source": [
    "#### VAE loss traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5261640d-d60b-44bd-86df-892d2a61bd19",
   "metadata": {},
   "source": [
    "The following cells allow us to visualize the loss over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8150f5d8-8b5a-403e-a7df-687028e93817",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = valdo.VAE.load(vae_path + run_prefix + 'trained_vae.pkl')\n",
    "loss_array = np.array(vae.loss_train)\n",
    "plt.figure(figsize=(5,3.5))\n",
    "plt.plot(loss_array)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss term\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ec823-ee45-4261-9239-aced93af3d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=1, figsize=[6,8])\n",
    "ax = axs.reshape(-1)\n",
    "\n",
    "ax[0].plot(loss_array[:,0], label='Total Loss, Training')\n",
    "ax[0].plot(loss_array[:,3], label='Total Loss, Validation')\n",
    "ax[0].set_xlabel(\"Steps\")\n",
    "ax[0].grid()\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(loss_array[:,1], label='Negative Log Likelihood, Training')\n",
    "ax[1].plot(loss_array[:,4], label='Negative Log Likelihood, Validation')\n",
    "ax[1].set_xlabel(\"Steps\")\n",
    "ax[1].grid()\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(loss_array[:,2], label='KL Divergence, Training')\n",
    "ax[2].plot(loss_array[:,5], label='KL Divergence, Validation')\n",
    "ax[2].set_xlabel(\"Steps\")\n",
    "ax[2].grid()\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ea7b78-bb2d-4709-af00-44b3901539ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Steps 5 & 6: Reconstruction of \"Apo\" Data & Calculating Difference Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62471f2c-4673-4ab1-b3a9-231ac0c93849",
   "metadata": {},
   "source": [
    "In this step, VAE outputs are re-scaled accordingly to recover the original scale, and differences in amplitudes between the original and reconstructed data are calculated. A `recons` and a `diff` column will be created for all datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de80754-e5c3-47d2-a4f0-32c344cbc9f9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb463176-561c-4c21-9934-5928074d1be3",
   "metadata": {},
   "source": [
    "To perform the reconstruction, or re-scaling, the `rescale()` function can be called, providing the necessary arguments:\n",
    "\n",
    "- `recons_path`: Path to the reconstructed output of the VAE in NumPy format.\n",
    "- `intersection_path`: Path to the pickle file containing the intersection of all scaled datasets.\n",
    "- `union_path`: Path to the pickle file containing the union data of all scaled datasets.\n",
    "- `input_files`: List of input file paths. This list should be in the same order as is in the `vae_input.npy` or `intersection.mtz`.\n",
    "- `info_path`: Path to the folder containing files with the mean and SD used for standardization previously.\n",
    "- `output_path`: Path to the folder where the reconstructed data will be saved.\n",
    "- `amplitude_col`: Column in the MTZ file that contains structure factor amplitudes to calculate the difference column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b453bd09-674b-40d2-9d29-3c47a1397014",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Code\n",
    "_Loads its inputs, saves its outputs._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a68d24-8676-4432-9bbb-ecd773f382cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained VAE\n",
    "vae = valdo.VAE.load(vae_path + run_prefix + 'trained_vae.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b5ae3e-b02b-4ade-807a-1a9852fa080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input file and create a tensor\n",
    "\n",
    "vae_input = np.load(vae_path + 'vae_input.npy')\n",
    "vae_input_tensor = torch.tensor(vae_input)\n",
    "if bGPU:\n",
    "    vae_input_tensor = vae_input_tensor.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e34fa3-7d56-4061-acaa-772d1577c7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the input file via VAE, convert to numpy, and save\n",
    "recons = vae.reconstruct(vae_input_tensor, ml_recon=ml_recon, repeats=vae_samples)\n",
    "if vae_samples > 1:\n",
    "    recons_list = []\n",
    "    for item in recons:\n",
    "        recons_list.append(item.detach().cpu().numpy())\n",
    "    recons_3d = np.array(recons_list)\n",
    "    recons=np.array([np.mean(recons_3d,axis=0), np.std(recons_3d,axis=0)])\n",
    "else:\n",
    "    recons = recons.detach().cpu().numpy()\n",
    "    \n",
    "np.save(vae_reconstructed_path + 'recons', recons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622dd5cf-b8b6-4b4f-a627-9992090d2f06",
   "metadata": {},
   "source": [
    "In the following step we put the reconstructed amplitudes back on the same scale as the input data (but _scaled to the reference MTZ_). \n",
    "\n",
    "This step can crash the kernel/session if not enough memory is available. In that case, try reducing ncpu below the available number of CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c5ef8-1c50-49f2-9fac-b093d4be808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Re-scale the reconstructed files accordingly and creates the `diff` column\n",
    "# Function is valdo.preprocessing.rescale\n",
    "\n",
    "ncpu_temp=10#ncpu/2\n",
    "\n",
    "with open(os.path.join(vae_path, 'filtered_file_list.pkl'),'rb') as f:\n",
    "    file_list = pickle.load(f)\n",
    "\n",
    "print(len(file_list))\n",
    "\n",
    "if ncpu > 1:\n",
    "    valdo.preprocessing.rescale_pool(recons_path=vae_reconstructed_path + 'recons.npy', \n",
    "                intersection_path=intersection_path, \n",
    "                union_path=union_path, \n",
    "                input_files=file_list, \n",
    "                info_folder=vae_path, \n",
    "                output_folder=vae_reconstructed_path,\n",
    "                amplitude_col=amplitude_scaled_col,\n",
    "                ncpu=ncpu_temp)\n",
    "else:\n",
    "    valdo.preprocessing.rescale(recons_path=vae_reconstructed_path + 'recons.npy', \n",
    "                intersection_path=intersection_path, \n",
    "                union_path=union_path, \n",
    "                input_files=file_list, \n",
    "                info_folder=vae_path, \n",
    "                output_folder=vae_reconstructed_path,\n",
    "                amplitude_col=amplitude_scaled_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e299c3d3-2425-4900-8536-397bd2cf1d2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Steps 7 & 8: Gaussian Blurring & Searching for Blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a39437b-916b-4b8c-8392-07cefffe6734",
   "metadata": {},
   "source": [
    "In this step, we aim to identify significant changes in electron density caused by ligand binding to a protein. By taking the absolute value of the electron density difference maps and applying Gaussian blurring, a new map is created with merged positive electron density blobs. The blurring process attempts to reduce noise. Blobs are then identified and characterized above a specified contour level and volume threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343d894-1c13-4dbf-8dbd-38a3880852ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e243ac5-1ca2-4a7b-9ccb-d9057de96617",
   "metadata": {},
   "source": [
    "To generate blobs from electron density maps, call the `generate_blobs()` function, which takes electron density map files and corresponding refined protein models as inputs. The function preprocesses the maps and identifies blobs above a specified contour level and volume threshold (the volume threshold is the default set by `gemmi`). The output is a DataFrame containing statistics for each identified blob, including peak value, score, centroid coordinates, volume, and radius. \n",
    "\n",
    "This function can be called with the following arguments:\n",
    "\n",
    "- `input_files`: List of input file paths.\n",
    "- `model_path`: Path to the folder containing the refined models for each dataset (pdb format).\n",
    "- `diff_col`: Name of the column representing diffraction values in the input MTZ files.\n",
    "- `phase_col`: Name of the column representing phase values in the input MTZ files.\n",
    "- `output_path`: Path to the output folder where the blob statistics DataFrame will be saved.\n",
    "- `cutoff`: Blob cutoff value. Blobs with values below this cutoff will be ignored (optional, default is `5`).\n",
    "- `negate`: Whether to negate the blob statistics (optional, default is `False`). Use True if there is interest in both positive and negative peaks, which is not typically of interest here due to the absolute value function applied to the map.\n",
    "- `sample_rate`: Sample rate for generating the grid in the FFT process (optional, default is `3`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d0496-da6d-49f3-b0bc-1d610ab58205",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Add Phases from Apo Refinement Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce772be-b46a-4d63-9836-1f6b13b41279",
   "metadata": {},
   "source": [
    "The following cells **add phases** to our newly reconstructed datasets. These phases are copied from `refined_path` which were generated via PHENIX in 0.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6af73e-66c7-4f3a-bd1a-0b3cd7758c45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(phasing_path)\n",
    "print(vae_reconstructed_with_phases_path)\n",
    "\n",
    "# List of reconstructed mtz files without phases to add phases to\n",
    "file_list = glob.glob(vae_reconstructed_path + \"*.mtz\")\n",
    "# file_list = file_list[:10]\n",
    "file_list_w_phases = file_list.copy()\n",
    "print(\"\\nWorking with \" + str(len(file_list)) + \" MTZs containing reconstructed (apo-like) amplitudes.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86ee2d4-3cab-49b8-b74b-e8511923fdef",
   "metadata": {},
   "source": [
    "#### Linking reconstructed amplitude MTZs to phases MTZs\n",
    "We need to know how to link the MTZ with VAE-reconstructed amplitudes to refinement MTZs containing phases. `valdo.helper.find_phase_file()` can try to do so, but it is best to provide a user-written parser to make sure this happens correctly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e477345-e4f4-4828-9c51-b5be398a6307",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(phasing_path)\n",
    "print(apo_phases_parser(file_list[0]))\n",
    "print(valdo.helper.find_phase_file(file_list[0], phasing_path, parser=apo_phases_parser))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fe3271-91fb-4e79-8f73-aed18bbbf9c3",
   "metadata": {},
   "source": [
    "#### adding phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c46eda-4cdc-41a8-9999-fef2dc9dd207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if ncpu > 1:\n",
    "    no_phases_files = valdo.helper.add_phases_pool(\n",
    "        file_list, \n",
    "        phasing_path, \n",
    "        vae_reconstructed_with_phases_path, \n",
    "        phase_2FOFC_col_out=phase_2FOFC_col_out, phase_FOFC_col_out=phase_FOFC_col_out,\n",
    "        phase_2FOFC_col_in =phase_2FOFC_col_in,  phase_FOFC_col_in =phase_FOFC_col_in,\n",
    "        prefix=run_prefix, \n",
    "        parser=apo_phases_parser,\n",
    "        ncpu=ncpu)\n",
    "    print(\"Done. No phases found for \" + str(len(no_phases_files)) + \" starting MTZs.\")\n",
    "else:\n",
    "    no_phases_files = valdo.helper.add_phases(\n",
    "        file_list, \n",
    "        phasing_path, \n",
    "        vae_reconstructed_with_phases_path, \n",
    "        phase_2FOFC_col_out=phase_2FOFC_col_out, phase_FOFC_col_out=phase_FOFC_col_out,\n",
    "        phase_2FOFC_col_in =phase_2FOFC_col_in,  phase_FOFC_col_in =phase_FOFC_col_in,\n",
    "        parser=apo_phases_parser\n",
    "    )\n",
    "    print(\"Done. No phases found for \" + str(len(no_phases_files)) + \" starting MTZs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bb0cb7-32e3-4161-be24-6933289f0466",
   "metadata": {},
   "source": [
    "In the case of dimple refinement for PTP-1B the ones that are missing are the ones for which both indexing solutions gave a poor CC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade1fba-70f6-48d5-b0f0-5bb62565d020",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "file_list = glob.glob(vae_reconstructed_with_phases_path + \"*.mtz\")\n",
    "print(\"Working with \" + str(len(file_list)) + \" MTZs containing reconstructed (apo-like) amplitudes.\")\n",
    "\n",
    "result = valdo.helper.add_weights(file_list, \n",
    "                                  sigF_col=error_scaled_col, \n",
    "                                  diff_col=\"diff\",\n",
    "                                  sigdF_pct=w_pcts[0], \n",
    "                                  absdF_pct=w_pcts[1],\n",
    "                                  redo=True,\n",
    "                                  ncpu=ncpu)\n",
    "print(\"Added weights and weighted differences to \" + str(sum(result)) + \" MTZ files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1651d6e0-f791-45e0-8a30-bc5ebe7d3c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "file_list = glob.glob(vae_reconstructed_with_phases_path + \"*.mtz\")\n",
    "result=valdo.helper.extrapolate(file_list, \\\n",
    "                                F_col=amplitude_scaled_col, \\\n",
    "                                sigF_col=error_scaled_col, \\\n",
    "                                recons_col=\"recons\", \\\n",
    "                                sigF_recons_col=\"SIG_recons\", \\\n",
    "                                diff_col=\"diff\", \\\n",
    "                                wt_col=\"WT\", \\\n",
    "                                redo=True, \\\n",
    "                                weighted=False, \\\n",
    "                                extrapolate_factors=[2,4,8,16], \\\n",
    "                                ncpu=ncpu)\n",
    "print(\"Calculated extrapolated structure factor amplitudes for \" + str(sum(result)) + \" MTZ files.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5dfc24-edc3-4930-89c7-0ed1aadc3087",
   "metadata": {},
   "source": [
    "### Gaussian Blurring Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eba489-51a5-43c8-ac68-6c7efb4125fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following two cells complete gaussian blurring and blob searching. For the blurring, by default, he radius is set to `5A` with `sigma = 5/3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840accda-3fda-4f7e-8d43-40a13193c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of reconstructed mtz files (with phases) to identify blobs in\n",
    "# tmp=\"/n/holyscratch01/hekstra_lab/dhekstra/valdo-tests/pipeline_run2/vae/reconstructed_w_phases/\"\n",
    "\n",
    "# usually vae_reconstructed_with_phases_path rather than tmp\n",
    "file_list = glob.glob(vae_reconstructed_with_phases_path + \"*mtz\")\n",
    "print(\"Retrieving \" + str(len(file_list)) + \" files for blob analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d004e3b-88c9-41ec-b3cf-20847dff62ce",
   "metadata": {},
   "source": [
    "The following takes about 0.3 s/dataset on a single CPU for PTP1B and will scale with the number of voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b972c767-ac5e-460c-a032-a26d48fe1299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Function in valdo.blobs that generates a list of blobs\n",
    "tmp_ncpu=12\n",
    "if ncpu>1:\n",
    "    valdo.blobs.generate_blobs_pool(\n",
    "        input_files=file_list, \n",
    "        model_folder=refined_path, \n",
    "        diff_col='WDF', # DEFAULT is \"diff\"!!!!\n",
    "        phase_col='refine_PH2FOFCWT', \n",
    "        output_folder=blob_path, \n",
    "        prefix=run_prefix,\n",
    "        cutoff=blob_sig_cutoff,\n",
    "        radius_in_A=radius_in_A,\n",
    "        ncpu=tmp_ncpu)\n",
    "else:\n",
    "    valdo.blobs.generate_blobs(\n",
    "        input_files=file_list, \n",
    "        model_folder=refined_path, \n",
    "        diff_col='WDF', \n",
    "        phase_col='refine_PH2FOFCWT', \n",
    "        output_folder=blob_path, \n",
    "        prefix=run_prefix,\n",
    "        cutoff=blob_sig_cutoff,\n",
    "        radius_in_A=radius_in_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a148fd9d-8164-48bb-a703-7da86dde87c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 9: Identifying Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b7ce19-3844-41c8-b1c9-0c7a944b0e63",
   "metadata": {},
   "source": [
    "In this final step, the highest scoring blobs returned in the previous step can be analyzed individually. If the blob is plausibly a ligand, refinement with a ligand may be completed to determine whether or not the blob can be considered a \"hit.\"\n",
    "\n",
    "Blobs that are returned can be related to various other events, not just ligand binding. Examples may include ligand-induced conformational change (which would still indicate the presence of a ligand) or various other unrelated conformational changes, such as radiation damage or, for example for PTP1B, cysteine oxidation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2565c239-9c93-430c-a549-78ca304851e8",
   "metadata": {},
   "source": [
    "In the following example, we have also included the evaluation of our method, via AUC, in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4330dcd-7e7c-435b-b883-467d4b28e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "phyllis_dir=\"/n/holyscratch01/hekstra_lab/phyllis/\"\n",
    "bound_models_path=phyllis_dir + \"PTP1B_DK/all_bound_models_reindexed_v2/\"\n",
    "bound_models_path=\"/n/holyscratch01/hekstra_lab/dhekstra/phyllis/PTP1B_DK/all_bound_models_reindexed_v2\"\n",
    "bound_sample_ids =phyllis_dir + \"bound_sample_ids.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd438f14-ff36-4b18-bdc3-4935776b40e1",
   "metadata": {},
   "source": [
    "### Copy existing bound models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e958250-cfb1-4274-8ffb-3d51eb545444",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bSkip=True\n",
    "\n",
    "if not bSkip:\n",
    "    # Define the source and destination folders\n",
    "    source_path      = bound_models_path\n",
    "    destination_path = bound_models_standardized_path\n",
    "    \n",
    "    # Get a list of all files in the source folder\n",
    "    file_list = os.listdir(source_path)\n",
    "    # print(file_list)\n",
    "    \n",
    "    # Define a regular expression pattern to match the filenames\n",
    "    pattern = r\".*(\\d{4}).*.pdb\"\n",
    "    \n",
    "    # Iterate over each file in the source folder\n",
    "    for filename in tqdm(file_list):\n",
    "        # print(filename)\n",
    "        # Check if the file matches the pattern\n",
    "        match = re.match(pattern, filename)\n",
    "        if match:\n",
    "            # Extract the ID from the filename\n",
    "            id = match.group(1)\n",
    "            \n",
    "            # Define the new filename\n",
    "            new_filename = id + \".pdb\"\n",
    "            \n",
    "            # Construct the full source and destination paths\n",
    "            tmp_source_path = os.path.join(source_path, filename)\n",
    "            tmp_destination_path = os.path.join(destination_path, new_filename)\n",
    "            \n",
    "            # Copy the file to the destination folder with the new name\n",
    "            # print(source_path)\n",
    "            # print(destination_path)\n",
    "            try:\n",
    "                shutil.copy(tmp_source_path, tmp_destination_path)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        else:\n",
    "            print(\"No match for \" + filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2323e869-9ec5-48f3-86f6-2f0985eaa748",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tagging Blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d0a6a4-cc2a-4040-ad32-ba7c8f804f94",
   "metadata": {},
   "source": [
    "In this section, we tag and filter the blobs. We remove...\n",
    "\n",
    "- blobs that are duplicates (we occassionally have duplicate blobs due to an issue with gemmi's ASU mask)\n",
    "- blobs associated with the oxidation of `cys215`\n",
    "- blobs that belong to low quality samples (high r factors in refinement)\n",
    "- blobs that belong to samples with inconsistent data (in particular, Helen Ginn lists a few samples as hits without including a ligand in their bound state).\n",
    "\n",
    "We also label blobs that are close to ligand positions known from a previous PanDDA analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b2d69-ab5f-4c68-a953-86f74ba1ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_df = pd.read_pickle(blob_path + run_prefix + 'blob_stats.pkl')\n",
    "blob_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f79b39-9398-4235-aea6-2eb06b759527",
   "metadata": {
    "tags": []
   },
   "source": [
    "Tag Samples that are known to contain a ligand (``Bound''): (1 if bound, 0 otherwise). Based on a previous PanDDA analysis. You can skip this if no such analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3488259-cb23-4d7a-8bf4-d10f8da48c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bound_sample_ids) as f:\n",
    "    bound_samples = set([line.strip() for line in f])\n",
    "\n",
    "# Set the \"bound\" column based on whether or not each sample is in the bound samples list\n",
    "blob_df[\"bound\"] = blob_df[\"sample\"].apply(lambda x: 1 if x.split(\"_\")[0] in bound_samples else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc644be0-423e-4514-a8ed-c449bd24812f",
   "metadata": {},
   "source": [
    "In the case of PTP-1B, the catalytic cysteine, residue 215, is prone to oxidation with levels varying among datasets. To get rid of these false positives, we tag Samples within 3A of a Cys215 Atom (1 if within, 0 otherwise). You can use/adapt `valdo.tag.tag_blobs_around_seqid` more generally to omit certain regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bfa05c-337c-4aa6-a46a-a0b26c23ab8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#with one CPU, this takes about 40 sec\n",
    "\n",
    "blob_df = valdo.tag.tag_blobs_around_seqid(blob_df, \n",
    "                                           refined_path, \n",
    "                                           radius=5, # increased from 3\n",
    "                                           tag='cys215', \n",
    "                                           focal_seqid=215, \n",
    "                                           ncpu=ncpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41e2def-1c2b-4395-b448-3a2b12be3bd9",
   "metadata": {},
   "source": [
    "In the case of PTP-1B, we already had an analysis by Keedy et al in hand using PanDDA. Here we tag samples within `r` of a known LIG atom (1 if yes, 0 otherwise). Fortunately, in all their bound models, the ligand \"residue name\" was LIG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61046323-1918-4491-ad08-8b38274dab46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "blob_df = valdo.tag.tag_lig_blobs(blob_df, \n",
    "                                  bound_models_standardized_path,\n",
    "                                  ncpu=ncpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf32b713-d46b-48d8-91b9-621c2f974d76",
   "metadata": {},
   "source": [
    "We use GEMMI under the hood to restrict blobs to a single asymmetric unit. Sometimes, however, we still double-include blobs if they are not in the exact same position. Here we tag Blobs that are Duplicates of Other Blobs (a.k.a. Patch for Gemmi's ASU Mask Issue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85978d50-d318-4314-811b-214c1c3c54fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Identifies all possible cartesian coordinates after symmetry operations\n",
    "# blob_df[['fractional', 'all_possible_frac', 'all_possible_cart']] = blob_df.apply(determine_locations, args=(vae_reconstructed_with_phases_path,), axis=1)\n",
    "blob_df = valdo.tag.determine_locations(blob_df, vae_reconstructed_with_phases_path, ncpu=ncpu)\n",
    "# blob_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc0a7b0-1e17-4926-8ca8-24bb0dc6f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# <1 sec\n",
    "# Marks blobs as duplicates if they are within 1A of another blob in the same sample\n",
    "blob_df = valdo.tag.mark_duplicates(blob_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9f581c-6283-4705-87ca-bab26ed539da",
   "metadata": {},
   "source": [
    "Let's look at some summary statistics so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73633b48-d768-4ba7-ab87-44e1af19a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387ec19d-fd35-4c86-8dd7-2e13b9d6050f",
   "metadata": {},
   "source": [
    "Tag Blobs Belonging to Samples with Low cc after scalings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c165e2-1a22-4560-940f-3bd596af5ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case, no blobs are removed\n",
    "metrics_df = pd.read_pickle(scaled_path + run_prefix + \"scaling_metrics.pkl\")\n",
    "\n",
    "low_end_corr = metrics_df.loc[metrics_df['end_corr'] < min_cc_ref, 'file'].tolist()\n",
    "blob_df['low_end_corr'] = blob_df['sample'].isin(low_end_corr)\n",
    "print(\"Number of blobs with end_corr < \" + str(min_cc_ref) + \" = \" +str(blob_df['low_end_corr'].sum()))\n",
    "\n",
    "# Should be 0 since we did this already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c497905-2710-41ff-9262-371fa5847266",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_df.to_pickle(blob_path + run_prefix + 'blob_stats_tagged.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf44bc-bb63-49d8-9e0e-21e9bf1d055f",
   "metadata": {},
   "source": [
    "### Filter Blobs \n",
    "Remove blobs near Cys215, with high R_free, duplicates, and more as specified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1dc04-ae37-43bd-a002-2eeeba6d225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_df = pd.read_pickle(blob_path + run_prefix +'blob_stats_tagged.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b794a6d-7781-493a-be20-0eed978338c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_df['sample_id'] = blob_df[\"sample\"].apply(lambda x: x.split(\"_\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96327c3-f59a-4465-ac31-491c15182030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all samples where Helen Ginn does not include a bound state model\n",
    "# In this case, there are no blobs.\n",
    "hg_no_lig = ['0060', '1429', '1733', '1791', '0225', '0432', '0710']\n",
    "blob_df = blob_df[~blob_df['sample_id'].isin(hg_no_lig)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f2246-d2ac-408e-aab1-aed7e9d03c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Cys215 related blobs, and duplicates\n",
    "\n",
    "blob_df = blob_df[(blob_df['cys215']==0) & (blob_df['duplicate']==0)]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2debc4-96cb-477a-88eb-127b36cd8184",
   "metadata": {},
   "source": [
    "Let's look at the filtered set of blobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca34f668-57cc-43e9-a3ca-b5751036f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8741a6e-8ee9-45ba-bac7-2fc55686be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_df.to_pickle(blob_path + run_prefix+'filtered_blob_stats_tagged.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ca000-9f0b-477d-8ab0-ddfffce22e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_blobs = blob_df.sort_values(by=\"peakz\",axis=0,ascending=False)\n",
    "sorted_blobs.iloc[250:300,:][[\"sample\",\"peakz\",\"score\",\"bound\",\"ligand\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d38d122-b7fa-4ef9-8a00-23da59c285eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate AUC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980e42a1-ce69-4815-b205-a324a3e5523d",
   "metadata": {},
   "source": [
    "In this section, we take the list of filtered blobs and 1) determine the AUC and 2) plot the ROC curve. We use `score` as the metric by which we classify blobs  a higher blob score means a higher likelihood that the blob represents ligand-binding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75577c49-4b83-42fc-966b-e55ce46ea26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_blob_stats(path, name=''):\n",
    "    \n",
    "    blob_df = pd.read_pickle(path)\n",
    "    \n",
    "    # create ROC curve\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(blob_df[\"ligand\"], blob_df[\"score\"], pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=name)\n",
    "    display.plot()\n",
    "    \n",
    "    print(\"Total Number of Blobs:\", len(blob_df))\n",
    "    print(\"Total Number of Unique Samples:\", len(blob_df.drop_duplicates(subset='sample')))\n",
    "    \n",
    "    plt.savefig(os.path.dirname(path) + '/roc_curve.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1859f451-23a3-4ffa-a16e-9d8a50172f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(              blob_path + run_prefix + 'filtered_blob_stats_tagged.pkl')\n",
    "plot_roc_blob_stats(blob_path + run_prefix + 'filtered_blob_stats_tagged.pkl')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7ad07c-e2b3-48d7-82c1-1c83ce474631",
   "metadata": {},
   "source": [
    "##### Heavy atom peak heights\n",
    "See `vae_metric_heavy_atom_peak_value.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8f7366-c67d-4428-a760-f51e10051480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79deb146-4ddf-4300-ad56-98e4ff64d633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d2cb8-ddb9-4413-8c89-3458f4b74121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
