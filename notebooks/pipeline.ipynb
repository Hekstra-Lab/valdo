{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6a68262-e910-4e21-aaaf-1c74febbb0a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bb0d29-6931-4d41-80de-ce887b129ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import re\n",
    "import reciprocalspaceship as rs\n",
    "import os\n",
    "import gemmi\n",
    "import math\n",
    "import shutil\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df81d0e9-eeba-49be-825f-7f2993211003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import valdo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2010ec6f-a1b5-45d9-a70b-9fad5f2b6ff1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3303685-7006-4327-a960-eee685d21202",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data_path = '../../pipeline/data/original_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea503dfd-f0a4-483c-a105-7db2a899f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = '../../pipeline/'\n",
    "reindexed_path = basepath + 'data/reindexed/'\n",
    "scaled_path = basepath + 'data/scaled/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec25fe6d-e279-4f80-9695-f5065538f4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_path = scaled_path + 'intersection.pkl'\n",
    "union_path = scaled_path + 'union.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef1bf73-f012-4d81-bb5c-69aa835b9958",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_folder = basepath + 'vae/'\n",
    "vae_reconstructed_folder = vae_folder + 'reconstructed/'\n",
    "vae_reconstructed_with_phases_folder = vae_folder + 'reconstructed-phases/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b58ebe-a57b-494d-9b5e-8df58859ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_folder = vae_folder + 'blobs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40646782-5f70-4192-85ce-115f0dfe4362",
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitude_col = 'F-obs'\n",
    "amplitude_scaled_col = 'F-obs-scaled'\n",
    "\n",
    "error_col = 'SIGF-obs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade70b4-f194-43e9-b766-5947b8d9d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "recons_col = 'recons'\n",
    "diff_col = 'diff'\n",
    "phase_2FOFC_col = 'refine_PH2FOFCWT'\n",
    "phase_FOFC_col = 'refine_PHFOFCWT'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e101699-3d69-4bca-9033-1736845da4f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Step 1: Diffraction Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabe69ce-7272-4802-993c-293a08402662",
   "metadata": {},
   "source": [
    "The first step involves acquiring diffraction datasets in the `mtz` format. These datasets should follow a specific naming convention, where each file is named with a number followed by the `.mtz` extension (e.g., `01.mtz`, `02.mtz`, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb296c84-c04a-460f-b2fe-182c200a10e1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03092f4d-aac8-40f3-87e2-d9c0ced26fc3",
   "metadata": {},
   "source": [
    "1. Ensure that you have collected diffraction datasets in the `mtz` format.\n",
    "\n",
    "2. Organize the datasets with sequential numerical names (e.g., `01.mtz`, `02.mtz`, etc.).\n",
    "\n",
    "Following this naming convention will allow datasets to be ready for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4333b1cb-c634-41ba-8cc7-36e5dac8d2b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Template Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a176fe-9487-4818-9a35-d204fe629da1",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following cell is a template for renaming files to the correct naming convention. Change `source_folder`, `destination_folder`, and extensions as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f1bf90-c93a-444a-ad2b-41cbbeff805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the source and destination folders\n",
    "source_folder = \"../../PTP1B_DK/all_bound_models_reindexed/\"\n",
    "destination_folder = \"../../pipeline/data/bound_models/\"\n",
    "\n",
    "# Get a list of all files in the source folder\n",
    "file_list = os.listdir(source_folder)\n",
    "\n",
    "# Define a regular expression pattern to match the filenames\n",
    "pattern = r\".*(\\d{4}).*.pdb\"\n",
    "\n",
    "# Iterate over each file in the source folder\n",
    "for filename in file_list:\n",
    "\n",
    "    # Check if the file matches the pattern\n",
    "    match = re.match(pattern, filename)\n",
    "    if match:\n",
    "        # Extract the ID from the filename\n",
    "        id = match.group(1)\n",
    "        \n",
    "        # Define the new filename\n",
    "        new_filename = id + \".pdb\"\n",
    "        \n",
    "        # Construct the full source and destination paths\n",
    "        source_path = os.path.join(source_folder, filename)\n",
    "        destination_path = os.path.join(destination_folder, new_filename)\n",
    "        \n",
    "        # Copy the file to the destination folder with the new name\n",
    "        shutil.copy(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd88a92-9666-43e3-a1dc-e685bba103a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Step 2: Reindexing & Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039e036e-c6c0-4283-a5f6-eef58a8dd678",
   "metadata": {},
   "source": [
    "This step focuses on reindexing and scaling a list of input MTZ files to a reference MTZ file using gemmi. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0945eb2d-c9a7-481a-b162-712d1cafbd09",
   "metadata": {},
   "source": [
    "**Reindexing:** The datasets provided may include samples from different space groups that describe the same physical crystal structure. To ensure comparability, we reindex each sample to a common indexing scheme by applying reindexing operators. \n",
    "\n",
    "**Scaling:** The samples are scaled to a reference dataset using a global anisotropic scaling factor by an analytical scaling method that determines the Debye-Waller Factor. The scaling process ensures that structure factor amplitudes are comparable across different datasets, accounting for variabilities such as differences in lattice orientations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8bb40e-7255-4066-9b08-a21e293c2211",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08a5d0-92aa-4f96-8e03-2476910481b6",
   "metadata": {},
   "source": [
    "1. Import the required library, `valdo`.\n",
    "\n",
    "2. Call the `reindex_files()` function from `valdo.reindex`. The `reindex_files()` function will enumerate possible reindexing operations for any space group and apply them to each input MTZ file. It will select the operation with the highest correlation with the reference dataset. The reindexed files will be saved in the specified output folder, following the same `##.mtz` naming convention.\n",
    "\n",
    "    This function can be called with the following parameters:\n",
    "    - `input_files`: List of paths to input MTZ files to be reindexed.\n",
    "    - `reference_file`: Path to the reference MTZ file.\n",
    "    - `output_folder`: Path to the folder where the reindexed MTZ files will be saved.\n",
    "    - `columns`: A list containing the names of the columns in the dataset that represent the amplitude and the error column.\n",
    "\n",
    "3. Create a `Scaler` object by providing the path to the reference MTZ file.\n",
    "\n",
    "4. Call the `batch_scaling()` method of the `Scaler` object. The `batch_scaling()` method will apply the scaling process to each input MTZ file and save the scaled MTZ files in the specified output folder. Scaling metrics, such as least squares values and correlations, will be saved in the report file.\n",
    "\n",
    "    This function can be called with the following parameters:\n",
    "    - `mtz_path_list`: List of paths to input MTZ files to be scaled.\n",
    "    - `outputmtz_path`: Path to the folder where the scaled MTZ files will be saved (optional, default is `./scaled_mtzs/`).\n",
    "    - `reportfile`: Path to the file where scaling metrics will be saved (optional, default is `./scaling_data.json`).\n",
    "    - `verbose`: Whether to display verbose information during scaling (optional, default is `True`).\n",
    "    - `n_iter`: Number of iterations for the analytical scaling method (optional, default is `5`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f13e11-7b4d-4845-84b0-b59451f6f3db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Reindexing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de3e185-789c-4768-adfd-25632d3baeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of files to be reindexed\n",
    "\n",
    "file_list = glob.glob(original_data_path + \"*mtz\")\n",
    "file_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49b0ae8-60a8-4baf-9e34-dc6499776ec7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reindexes a list of input MTZ files to a reference MTZ file using gemmi\n",
    "\n",
    "valdo.reindex.reindex_files(input_files=file_list, \n",
    "              reference_file=file_list[0], \n",
    "              output_folder=reindexed_path,\n",
    "              columns=[amplitude_col, error_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d117a7e-357a-4008-aed9-5e2fa2129726",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Scaling Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0d95fe-1efa-4f7d-81b8-aeed41c38372",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(reindexed_path + \"*mtz\")\n",
    "file_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a1b86f-1b75-434f-82df-c558eb69c341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the Scaler, file_list[0] serves as the reference\n",
    "\n",
    "scaler = valdo.Scaler(reference_mtz=file_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d989673-1405-4abe-b168-7fbdda97c406",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scales all datasets to the previously provided reference, writes a `metrics.pkl`\n",
    "\n",
    "metrics = scaler.batch_scaling(mtz_path_list=file_list, \n",
    "                               outputmtz_path=scaled_path, \n",
    "                               verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c24816-8832-4a38-953e-e0e02df2b4ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Step 3: Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4c5cf6-11a1-45f1-bc60-cb64aa6b7b06",
   "metadata": {},
   "source": [
    "This step involves normalizing the scaled structure factor amplitudes obtained in the previous step. The input is restricted to only those Miller indices present in the intersection of all datasets, and the VAE predicts structure factor amplitudes for all Miller indices in the union of all datasets.\n",
    "\n",
    "Additionally, we standardize all the input data, such that the structure factor amplitudes for each Miller index in the union of all datasets have a mean of zero and a unit variance across datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e23f578-3067-43a8-a3b8-c78fbcdb4399",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9769802-6333-4d7b-adab-37a54da31847",
   "metadata": {},
   "source": [
    "1. Import the required library, `valdo.preprocessing`.\n",
    "\n",
    "2. Find the intersection and union of the scaled datasets using the following functions:\n",
    "\n",
    "   - `find_intersection()`: Finds the intersection of `amplitude_col` from multiple input MTZ files and saves the result to the specified output pickle file. Arguments include the following:\n",
    "\n",
    "      - `input_files`: List of input MTZ file paths.\n",
    "      - `output_path`: Path to save the output pickle file containing the intersection data.\n",
    "      - `amplitude_col`: Name of the column in the dataset that represents the scaled amplitude (default is 'F-obs-scaled').\n",
    "\n",
    "   - `find_union()`: Finds the union of `amplitude_col` from multiple input MTZ files and saves the result to the specified output pickle file. Arguments are the same as `find_intersection()`.\n",
    "\n",
    "3. Generate the VAE input and output data using the `generate_vae_io()` function. This standardizes the intersection dataset using mean and standard deviation calculated from the union dataset. The standardized intersection becomes the VAE input, while the standardized union becomes the VAE output. Both the VAE input and output are saved to the specified folder. \n",
    "\n",
    "    This function can be called with the following parameters:\n",
    "\n",
    "    - `intersection_path`: Path to the intersection dataset pickle file.\n",
    "    - `union_path`: Path to the union dataset pickle file.\n",
    "    - `io_folder`: Path to the output folder where the VAE input and output will be saved. Mean and standard deviation data calculated from the union dataset will also be saved in this folder as `union_mean.pkl` and `union_sd.pkl`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deafcf0-7551-4c81-b9cc-dfdc7f66c3dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bdb137-e08c-48bd-8166-a577f5b12c05",
   "metadata": {},
   "source": [
    "In this example, we remove samples with low `end_corr`. This ensures that our VAE is trained with high quality samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dbb815-398f-4797-a191-ea9ed6a8bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all scaled files to use as input and output for the VAE\n",
    "\n",
    "file_list = glob.glob(scaled_path + \"*mtz\")\n",
    "file_list.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ab1fd-9e96-4dcd-9572-010c5022df9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "This following cell removes samples with `end_corr < 0.6` or if `end_corr = NA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ad07d-e684-4aed-b568-9117ab062cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_pickle(scaled_path + \"metrics.pkl\")\n",
    "metrics_df.columns=['file', 'start_LS', 'start_corr', 'end_LS', 'end_corr']\n",
    "metrics_df[metrics_df.isna().any(axis=1)]\n",
    "low_corr_files = list(metrics_df[(metrics_df['end_corr'] < 0.6) | (metrics_df['end_corr'].isnull())]['file'])\n",
    "low_corr_files = [scaled_path + x + '.mtz' for x in low_corr_files]\n",
    "file_list = [file for file in file_list if file not in low_corr_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94831fcd-f842-4c37-96ec-3e750a245b45",
   "metadata": {},
   "source": [
    "The following cells generate the VAE input and output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64475a6-ae79-4136-81da-d955fe548e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an `intersection.mtz` file at the specified path\n",
    "# This is the intersection of all the scaled files provided\n",
    "\n",
    "valdo.preprocessing.find_intersection(input_files=file_list, \n",
    "                  output_path=intersection_path,\n",
    "                  amplitude_col=amplitude_scaled_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ec5d5a-d74f-4730-ad5c-8a25b4823fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an `union.mtz` file at the specified path\n",
    "# This is the union of all the scaled files provided\n",
    "\n",
    "valdo.preprocessing.find_union(input_files=file_list, \n",
    "           output_path=union_path,\n",
    "           amplitude_col=amplitude_scaled_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f2c8d-3d30-414b-9c68-c194dbc9b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates VAE input and output data from the intersection and union datasets\n",
    "\n",
    "valdo.preprocessing.generate_vae_io(intersection_path=intersection_path, \n",
    "                union_path=union_path, \n",
    "                io_folder=vae_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20af9fe-70d3-4683-b199-c4ee0d8b946f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Step 4: VAE Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bbe4f6-6158-403c-855e-a9ae78a9a00b",
   "metadata": {},
   "source": [
    "In this step, we train the VAE model using the provided VAE class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c3057-4c5c-43fd-94bc-fc718affe32e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a414ee3a-0bc9-4344-8ab4-1090ac02eb68",
   "metadata": {},
   "source": [
    "1. Load the VAE input and output data that was generated in the previous step.\n",
    "\n",
    "2. Initialize the VAE model with the desired hyperparameters. Tune-able hyperparameters include the following:\n",
    "    - `n_dim_latent`: Number of dimensionality in latent space (optional, default `1`)\n",
    "\n",
    "    - `n_hidden_layers`: Number of hidden layers in the encoder and decoder. If an int is given, it will applied to both encoder and decoder; If a length 2 list is given, first int will be used for encoder, the second will be used for decoder\n",
    "\n",
    "    - `n_hidden_size`: Number of units in hidden layers. If an int is given, it will be applied to all hidden layers in both encoder and decoder; otherwise, an array with length equal to the number of hidden layers can be given, the number of units will be assigned accordingly.\n",
    "\n",
    "    - `activation` : Activation function for the hidden layers (optional, default `tanh`)\n",
    "\n",
    "3. Split the data into training and validation sets. Randomly select a subset of indices for training and use the rest for validation.\n",
    "\n",
    "4. Convert the data into PyTorch tensors.\n",
    "\n",
    "5. Set up the optimizer for training.\n",
    "\n",
    "6. Train the VAE model using the `train()` method. The training process involves minimizing the ELBO (Evidence Lower Bound) loss function, which consists of a Negative Log-Likelihood (NLL) term and a Kullback-Leibler (KL) divergence term. Arguments used in this function include:\n",
    "\n",
    "    - `x_train`: Input data for training the VAE, a PyTorch tensor representing the VAE input data. \n",
    "\n",
    "    - `y_train`: Output data for training the VAE, a PyTorch tensor representing the VAE output data. \n",
    "\n",
    "    - `optim`: The optimizer used for training the VAE, a PyTorch optimizer object, such as `torch.optim.Adam`, that specifies the optimization algorithm and its hyperparameters, including the learning rate (`lr`).\n",
    "\n",
    "    - `x_val`: Input data for validation during training. (optional, default is `None`).\n",
    "\n",
    "    - `y_val`: Output data for validation during training. (optional, default is `None`).\n",
    "\n",
    "    - `epochs`: The number of training epochs (epoch: a single pass through the data).\n",
    "\n",
    "    - `batch_size`: The batch size used during training. If an integer is provided, the same batch size will be used for all epochs. If a list of integers is provided, it should have the same length as the number of epochs, and each value in the list will be used as the batch size for the corresponding epoch. Default is `256`.\n",
    "\n",
    "    - `w_kl`: The weight of the Kullback-Leibler (KL) divergence term in the ELBO loss function. The KL divergence term encourages the latent distribution to be close to a prior distribution (usually a standard normal distribution). A higher value of `w_kl` will increase the regularization strength on the latent space. Default is `1.0`.\n",
    "\n",
    "    **Note:** The VAE class internally keeps track of the training loss (`loss_train`) and its components (NLL and KL divergence) during each batch of training. These values can be accessed after training to monitor the training progress and performance. The `loss_train` attribute of the VAE object will be a list containing the training loss values for each batch during training. The `loss_names` attribute contains the names of the loss components: \"Loss\", \"NLL\", and \"KL_div\". These attributes are updated during training and can be used for analysis or visualization.\n",
    "\n",
    "7. Save the trained VAE model for future use (optional)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd901df-c30b-4d73-95a3-79e18491136e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f672b-47ce-464c-997b-3ed9ececd1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VAE I/O Files Generated\n",
    "\n",
    "vae_input = np.load(vae_folder + 'vae_input.npy')\n",
    "vae_output = np.load(vae_folder + 'vae_output.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc21004e-7cfb-4ee8-bd36-377b10f7adf5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify VAE Parameters\n",
    "latent_dimension = 7\n",
    "\n",
    "vae = valdo.VAE(n_dim_i = vae_input.shape[1], \n",
    "      n_dim_o = vae_output.shape[1], \n",
    "      n_dim_latent = latent_dimension, \n",
    "      n_hidden_layers = [3, 6], \n",
    "      n_hidden_size = 100, \n",
    "      activation = torch.relu)\n",
    "\n",
    "# Randomly select 1300 indices for training\n",
    "choice = np.random.choice(vae_input.shape[0], 1300, replace=False)    \n",
    "train_ind = np.zeros(vae_input.shape[0], dtype=bool)\n",
    "train_ind[choice] = True\n",
    "test_ind = ~train_ind\n",
    "\n",
    "# Split the input and output data into training and validation sets\n",
    "x_train, x_val = vae_input[train_ind], vae_input[test_ind]\n",
    "y_train, y_val = vae_output[train_ind], vae_output[test_ind]\n",
    "\n",
    "# Convert the data to torch tensors\n",
    "x_train, x_val, y_train, y_val = torch.tensor(x_train), torch.tensor(x_val), torch.tensor(y_train), torch.tensor(y_val)\n",
    "\n",
    "# Set up the optimizer and train the VAE\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "vae.train(x_train, y_train, optimizer, x_val, y_val, epochs=300, batch_size=100, w_kl=1.0)\n",
    "\n",
    "# Save the trained VAE model\n",
    "vae.save(vae_folder + 'trained_vae.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5261640d-d60b-44bd-86df-892d2a61bd19",
   "metadata": {},
   "source": [
    "The following cells allow us to visualize the loss over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8150f5d8-8b5a-403e-a7df-687028e93817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss over time\n",
    "\n",
    "vae = valdo.VAE.load(vae_folder + 'trained_vae.pkl')\n",
    "loss_array = np.array(vae.loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ec823-ee45-4261-9239-aced93af3d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=1, figsize=[10,12])\n",
    "ax = axs.reshape(-1)\n",
    "\n",
    "ax[0].plot(loss_array[:,0], label='Total Loss, Training')\n",
    "ax[0].plot(loss_array[:,3], label='Total Loss, Validation')\n",
    "ax[0].set_xlabel(\"Steps\")\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(loss_array[:,1], label='Negative Log Likelihood, Training')\n",
    "ax[1].plot(loss_array[:,4], label='Negative Log Likelihood, Validation')\n",
    "ax[1].set_xlabel(\"Steps\")\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(loss_array[:,2], label='KL Divergence, Training')\n",
    "ax[2].plot(loss_array[:,5], label='KL Divergence, Validation')\n",
    "ax[2].set_xlabel(\"Steps\")\n",
    "ax[2].legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ea7b78-bb2d-4709-af00-44b3901539ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Steps 5 & 6: Reconstruction of \"Apo\" Data & Calculating Difference Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62471f2c-4673-4ab1-b3a9-231ac0c93849",
   "metadata": {},
   "source": [
    "In this step, VAE outputs are re-scaled accordingly to recover the original scale, and differences in amplitudes between the original and reconstructed data are calculated. A `recons` and a `diff` column will be created for all datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de80754-e5c3-47d2-a4f0-32c344cbc9f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb463176-561c-4c21-9934-5928074d1be3",
   "metadata": {},
   "source": [
    "To perform the reconstruction, or re-scaling, the `rescale()` function can be called, providing the necessary arguments:\n",
    "\n",
    "- `recons_path`: Path to the reconstructed output of the VAE in NumPy format.\n",
    "- `intersection_path`: Path to the pickle file containing the intersection of all scaled datasets.\n",
    "- `union_path`: Path to the pickle file containing the union data of all scaled datasets.\n",
    "- `input_files`: List of input file paths. This list should be in the same order as is in the `vae_input.npy` or `intersection.mtz`.\n",
    "- `info_folder`: Path to the folder containing files with the mean and SD used for standardization previously.\n",
    "- `output_folder`: Path to the folder where the reconstructed data will be saved.\n",
    "- `amplitude_col`: Column in the MTZ file that contains structure factor amplitudes to calculate the difference column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b453bd09-674b-40d2-9d29-3c47a1397014",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a68d24-8676-4432-9bbb-ecd773f382cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained VAE\n",
    "\n",
    "vae = valdo.VAE.load(vae_folder + 'trained_vae.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b5ae3e-b02b-4ade-807a-1a9852fa080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input file and create a tensor\n",
    "\n",
    "vae_input = np.load(vae_folder + 'vae_input.npy')\n",
    "vae_input_tensor = torch.tensor(vae_input)\n",
    "vae_input_tensor = vae_input_tensor.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e34fa3-7d56-4061-acaa-772d1577c7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the input file via VAE, convert to numpy, and save\n",
    "\n",
    "recons = vae.reconstruct(torch.tensor(vae_input_tensor))\n",
    "recons = recons.detach().cpu().numpy()\n",
    "np.save(vae_reconstructed_folder + 'recons', recons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c5ef8-1c50-49f2-9fac-b093d4be808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-scale the reconstructed files accordingly and creates the `diff` column\n",
    "# Function is valdo.preprocessing.rescale\n",
    "\n",
    "valdo.preprocessing.rescale(recons_path=vae_reconstructed_folder + 'recons.npy', \n",
    "            intersection_path=intersection_path, \n",
    "            union_path=union_path, \n",
    "            input_files=file_list, \n",
    "            info_folder=vae_folder, \n",
    "            output_folder=vae_reconstructed_folder,\n",
    "            amplitude_col=\"F-obs-scaled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e299c3d3-2425-4900-8536-397bd2cf1d2f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Steps 7 & 8: Gaussian Blurring & Searching for Blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58f7cdf-536d-46cd-8252-3d9d35402e15",
   "metadata": {},
   "source": [
    "**Note Regarding Phases:** In this section, phases are required for each dataset. You can obtain phases by completing refinement via PHENIX for each dataset, and utilizing those phases.\n",
    "\n",
    "**Note Regarding Models:** In this section, models are also required for each dataset. These can also be obtained by refinement via PHENIX for each dataset, and they should be stored in a single folder, with the same naming convention (i.e. ##.mtz).\n",
    "\n",
    "We offer a command-line tool for automatic refinement using PHENIX. Based on our tests, starting with a single apo model yields satisfactory phases and models for the following real-space maps. You can find an example refine_drug.eff file in the notebook/ directory.\n",
    "\n",
    "*Code Example:* `valdo.refine --pdbpath \"xxx/xxx_apo.pdb\" --mtzpath \"xxx/*.mtz\" --output \"yyy/\" --eff \"xxx/refine_drug.eff\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a39437b-916b-4b8c-8392-07cefffe6734",
   "metadata": {},
   "source": [
    "In this step, we aim to identify significant changes in electron density caused by ligand binding to a protein. By taking the absolute value of the electron density difference maps and applying Gaussian blurring, a new map is created with merged positive electron density blobs. The blurring process attempts to reduce noise. Blobs are then identified and characterized above a specified contour level and volume threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343d894-1c13-4dbf-8dbd-38a3880852ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e243ac5-1ca2-4a7b-9ccb-d9057de96617",
   "metadata": {},
   "source": [
    "To generate blobs from electron density maps, call the `generate_blobs()` function, which takes electron density map files and corresponding refined protein models as inputs. The function preprocesses the maps and identifies blobs above a specified contour level and volume threshold (the volume threshold is the default set by `gemmi`). The output is a DataFrame containing statistics for each identified blob, including peak value, score, centroid coordinates, volume, and radius. \n",
    "\n",
    "This function can be called with the following arguments:\n",
    "\n",
    "- `input_files`: List of input file paths.\n",
    "- `model_folder`: Path to the folder containing the refined models for each dataset (pdb format).\n",
    "- `diff_col`: Name of the column representing diffraction values in the input MTZ files.\n",
    "- `phase_col`: Name of the column representing phase values in the input MTZ files.\n",
    "- `output_folder`: Path to the output folder where the blob statistics DataFrame will be saved.\n",
    "- `cutoff`: Blob cutoff value. Blobs with values below this cutoff will be ignored (optional, default is `5`).\n",
    "- `negate`: Whether to negate the blob statistics (optional, default is `False`). Use True if there is interest in both positive and negative peaks, which is not typically of interest here due to the absolute value function applied to the map.\n",
    "- `sample_rate`: Sample rate for generating the grid in the FFT process (optional, default is `3`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d0496-da6d-49f3-b0bc-1d610ab58205",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce772be-b46a-4d63-9836-1f6b13b41279",
   "metadata": {},
   "source": [
    "The following cell adds phases to our newly reconstructed datasets. These phases are copied from `../../pipeline/data/refined_mtzs/` which were generated via PHENIX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf42787d-1e22-4caa-ba9f-284f27ee5119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of reconstructed mtz files without phases to add phases to\n",
    "\n",
    "file_list = glob.glob(vae_reconstructed_folder + \"*mtz\")\n",
    "\n",
    "no_phases_files = []\n",
    "\n",
    "# Phases here are copied from refinement \n",
    "\n",
    "for file in tqdm(file_list):\n",
    "    \n",
    "    current = rs.read_mtz(file)\n",
    "    \n",
    "    try:\n",
    "        phases_df = rs.read_mtz('../../pipeline/data/refined_mtzs/'+os.path.basename(file))\n",
    "\n",
    "    except:\n",
    "        no_phases_files.append(file)\n",
    "        continue\n",
    "    \n",
    "    current[phase_2FOFC_col] = phases_df['PH2FOFCWT']\n",
    "    current[phase_FOFC_col] = phases_df['PHFOFCWT']\n",
    "    \n",
    "    current.write_mtz(vae_reconstructed_with_phases_folder + os.path.basename(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eba489-51a5-43c8-ac68-6c7efb4125fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following two cells complete gaussian blurring and blob searching. For the blurring, the radius is set to `5A` with `sigma = 5/3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840accda-3fda-4f7e-8d43-40a13193c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of reconstructed mtz files (with phases) to identify blobs in\n",
    "\n",
    "file_list = glob.glob(vae_reconstructed_with_phases_folder + \"*mtz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b972c767-ac5e-460c-a032-a26d48fe1299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function in valdo.blobs that generates a list of blobs\n",
    "\n",
    "valdo.blobs.generate_blobs(input_files=file_list, \n",
    "               model_folder='../../pipeline/data/refined_models/', \n",
    "               diff_col='diff', \n",
    "               phase_col='refine_PH2FOFCWT', \n",
    "               output_folder=blob_folder, \n",
    "               cutoff=3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a148fd9d-8164-48bb-a703-7da86dde87c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Step 9: Identifying Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b7ce19-3844-41c8-b1c9-0c7a944b0e63",
   "metadata": {},
   "source": [
    "In this final step, the highest scoring blobs returned in the previous step can be analyzed individually. If the blob is plausibly a ligand, refinement with a ligand may be completed to determine whether or not the blob can be considered a \"hit.\"\n",
    "\n",
    "Blobs that are returned can be related to various other events, not just ligand binding. Examples may include ligand-induced conformational change (which would still indicate the presence of a ligand) or various other unrelated conformational changes, such as radiation damage or cysteine oxidation (as is seen in `pipeline.ipynb`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2565c239-9c93-430c-a549-78ca304851e8",
   "metadata": {},
   "source": [
    "In the following example, we have also included the evaluation of our method, via AUC, in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d638221-79af-4b2a-9271-c51ebdc32ddc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3a1070-7e9b-41ed-8102-9027dfa66d30",
   "metadata": {},
   "source": [
    "These functions will help us do various tasks. \n",
    "\n",
    "For example, we may want to remove blobs that are associated with `cys215` oxidation, as in PTP1B, the oxidation of `cys215` modulates the protein's activity.\n",
    "\n",
    "Additionally, to evaluate our method, we have a function that tags each blob to identify whether or not it is a blob associated with a ligand. We can then calculate the AUC. \n",
    "\n",
    "Various other helpful functions are also included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469d3343-31ad-47ac-96f7-f7d33d488a3a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Finds Nearby Atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c36b0-4b1d-4813-bb42-e3075bc85740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearby_atoms(centroid_dict, structure_path, sample_no=None, radius=3):\n",
    "    \n",
    "    \"\"\"\n",
    "    Finds nearby atoms within a specified radius around a given centroid position in a structure file and returns the atom details as a DataFrame.\n",
    "\n",
    "    The function reads the structure file in PDB format using gemmi, performs a neighbor search within the specified radius around the centroid position, and retrieves information about the nearby atoms. The atom details include the sample number, chain name, residue sequence ID, residue name, atom name, element name, coordinates (x, y, z), and distance from the centroid.\n",
    "\n",
    "    Args:\n",
    "        centroid_dict (dict): Dictionary containing the centroid position with keys 'x', 'y', and 'z'.\n",
    "        structure_path (str): Path to the structure file in PDB format.\n",
    "        sample_no (str): Sample number or identifier. Optional.\n",
    "        radius (float, optional): Radius in angstroms to search for nearby atoms. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing the details of the nearby atoms.\n",
    "\n",
    "    Example:\n",
    "        centroid = {'x': 10.0, 'y': 20.0, 'z': 30.0}\n",
    "        structure_file = './data/structure.pdb'\n",
    "        sample_number = 'S001'\n",
    "        nearby_atoms = find_nearby_atoms(centroid, structure_file, sample_number, radius=5)\n",
    "    \"\"\"\n",
    "    \n",
    "    peaks = []\n",
    "    \n",
    "    structure = gemmi.read_pdb(structure_path)\n",
    "    ns = gemmi.NeighborSearch(structure[0], structure.cell, radius).populate()\n",
    "    centroid = gemmi.Position(centroid_dict[\"x\"], centroid_dict[\"y\"], centroid_dict[\"z\"])\n",
    "    marks = ns.find_atoms(centroid)\n",
    "    \n",
    "    for mark in marks:\n",
    "        image_idx = mark.image_idx\n",
    "        cra = mark.to_cra(structure[0])\n",
    "        dist = structure.cell.find_nearest_pbc_image(centroid, cra.atom.pos, mark.image_idx).dist()\n",
    "\n",
    "        record = {\n",
    "            \"sample\"  :    sample_no,\n",
    "            \"chain\"   :    cra.chain.name,\n",
    "            \"seqid\"   :    cra.residue.seqid.num,\n",
    "            \"residue\" :    cra.residue.name,\n",
    "            \"atom\"    :    cra.atom.name,\n",
    "            \"element\" :    cra.atom.element.name,\n",
    "            \"coordx\"  :    cra.atom.pos.x,\n",
    "            \"coordy\"  :    cra.atom.pos.y,\n",
    "            \"coordz\"  :    cra.atom.pos.z,\n",
    "            \"dist\"    :    dist\n",
    "        }\n",
    "\n",
    "        peaks.append(record)\n",
    "        \n",
    "    return pd.DataFrame(peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90edbf18-dc95-46b7-a4e7-c36615173dcf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Tag Blobs near Cys215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7379344-2874-49d5-8ce8-d09a95151666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_cys_215_blobs(df, structure_path, radius=3):\n",
    "    \n",
    "    \"\"\"\n",
    "    Tags the blobs in the DataFrame 'df' that contain the CYS 215 residue based on the nearby atoms found in PDB files.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame containing the blobs information.\n",
    "        structure_path (str): The path to the folder containing the PDB files used for identifying nearby atoms.\n",
    "        radius (int, optional): The radius in Angstroms for finding nearby atoms. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The modified DataFrame with an additional 'cys215' column indicating the presence (1) or absence (0) of CYS 215 in the blobs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def check_blob_for_cys(row):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            row (pandas.Series): A row of the DataFrame representing a blob.\n",
    "\n",
    "        Returns:\n",
    "            int: Returns 1 if the blob contains CYS 215, otherwise returns 0.\n",
    "        \"\"\"\n",
    "        \n",
    "        sample = row[\"sample\"]\n",
    "        \n",
    "        cenx, ceny, cenz = row['cenx'], row['ceny'], row['cenz']\n",
    "        atoms_df = find_nearby_atoms({\"x\": cenx, \"y\": ceny, \"z\": cenz}, structure_path + sample + '.pdb', sample, radius)\n",
    "        \n",
    "        if len(atoms_df) < 1:\n",
    "            return 0\n",
    "        \n",
    "        if 215 in set(atoms_df['seqid']):\n",
    "            return 1\n",
    "        return 0\n",
    "    \n",
    "    tqdm.pandas()\n",
    "    df['cys215'] = df.progress_apply(check_blob_for_cys, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d10004b-e4ad-42e6-b2bc-ec8ac646a05b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Tag Blobs near LIG Atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a129d7-5e04-4250-ad33-52f82d263314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_lig_blobs(df, structure_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Tags the blobs in the DataFrame 'df' that contain ligands based on the nearby atoms found in PDB files.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The input DataFrame containing the blobs information.\n",
    "        structure_path (str): The path to the folder containing the PDB files used for identifying nearby atoms.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The modified DataFrame with an additional 'ligand' column indicating the presence (1) or absence (0) of ligands in the blobs.\n",
    "\n",
    "    \"\"\"\n",
    "    def check_blob_for_lig(row):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            row (pandas.Series): A row of the DataFrame representing a blob.\n",
    "\n",
    "        Returns:\n",
    "            int: Returns 1 if the blob contains ligands, otherwise returns 0.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if row[\"bound\"] == 0:\n",
    "            return 0\n",
    "        \n",
    "        sample = row[\"sample\"]\n",
    "        \n",
    "        cenx, ceny, cenz = row['cenx'], row['ceny'], row['cenz']\n",
    "        atoms_df = find_nearby_atoms({\"x\": cenx, \"y\": ceny, \"z\": cenz}, structure_path + sample + '.pdb', sample, row['radius'])\n",
    "\n",
    "        if len(atoms_df) < 1:\n",
    "            return 0\n",
    "        \n",
    "        if 'LIG' in set(atoms_df['residue']):\n",
    "            return 1\n",
    "        return 0\n",
    "    \n",
    "    tqdm.pandas()\n",
    "    df['ligand'] = df.progress_apply(check_blob_for_lig, axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65502959-34b4-4080-90a8-8ec4db8626df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Validate Fractional Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8612ecc7-2e14-4fdd-a994-4cc5b60792b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fractional_coords(coords):\n",
    "    \n",
    "    \"\"\"\n",
    "    Converts fractional coordinates to valid fractional coordinates within the range [0, 1).\n",
    "\n",
    "    Args:\n",
    "        coords (list or numpy.ndarray): The input fractional coordinates.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The converted valid fractional coordinates within the range [0, 1).\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    valid_coords = np.array(coords)\n",
    "    for i in range(3):\n",
    "        while valid_coords[i] > 1:\n",
    "            valid_coords[i] -= 1\n",
    "        while valid_coords[i] < 0:\n",
    "            valid_coords[i] += 1\n",
    "    return valid_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c35a3-f887-4f0f-b625-be46415fabbd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Determine Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c36dc-e4d7-41fd-ac18-3013901ce80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fractionalize coordinates and find all symmetry-related cartesian points \n",
    "\n",
    "def determine_locations(row, folder):\n",
    "    \n",
    "    \"\"\"\n",
    "    Converts coordinates to fractional form and determines all symmetry-related cartesian points for the given row.\n",
    "\n",
    "    Args:\n",
    "        row (pandas.Series): A row of the DataFrame representing a blob.\n",
    "        folder (str): The path to the folder containing the mtz files.\n",
    "\n",
    "    Returns:\n",
    "        pandas.Series: A pandas Series containing the fractional coordinates, all possible fractional coordinates, and all possible cartesian coordinates.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # find mtz file for sample number\n",
    "    mtz_file = folder + row['sample'] + '.mtz'\n",
    "    if mtz_file is None:\n",
    "        return pd.Series({'fractional': np.nan, 'all_possible_frac': np.nan, 'all_possible_cart': np.nan})\n",
    "    \n",
    "    # read in mtz file\n",
    "    sample_file = rs.read_mtz(mtz_file)\n",
    "    \n",
    "    # fractionalize coordinates using move2cell\n",
    "    frac_coords = move2cell([row['cenx'], row['ceny'], row['cenz']], sample_file.cell)\n",
    "    \n",
    "    # identify all symmetry operations\n",
    "    all_ops = list(sample_file.spacegroup.operations().sym_ops)\n",
    "\n",
    "    all_possible_frac = []\n",
    "    for op in all_ops:\n",
    "        result = op.apply_to_xyz(frac_coords)\n",
    "        result = valid_fractional_coords(result)\n",
    "        all_possible_frac.append(result)\n",
    "        \n",
    "    all_possible_frac = sorted(all_possible_frac, key=lambda x: x[0])\n",
    "                \n",
    "    # orthogonalize fractional coordinates\n",
    "    all_possible_cart = [sample_file.cell.orthogonalize(gemmi.Fractional(*elt)) for elt in all_possible_frac]\n",
    "    \n",
    "    all_possible_cart = [np.array([elt.x, elt.y, elt.z]) for elt in all_possible_cart]\n",
    "    \n",
    "    return pd.Series({'fractional': frac_coords, 'all_possible_frac': all_possible_frac, 'all_possible_cart': all_possible_cart})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af46b8d-524b-432f-85da-997d2413f09c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Move Points to the Unit Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05afa9-af0c-4368-bff9-25b5a9ed53da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move2cell(cartesian_coordinates, unit_cell, fractionalize=True):\n",
    "    \n",
    "    '''\n",
    "    Move your points into a unitcell with translational vectors\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cartesian_coordinates: array-like\n",
    "        [N_points, 3], cartesian positions of points you want to move\n",
    "        \n",
    "    unit_cell, gemmi.UnitCell\n",
    "        A gemmi unitcell instance\n",
    "    \n",
    "    fractionalize: boolean, default True\n",
    "        If True, output coordinates will be fractional; Or will be cartesians\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    array-like, coordinates inside the unitcell\n",
    "    '''\n",
    "    o2f_matrix = np.array(unit_cell.fractionalization_matrix)\n",
    "    frac_pos = np.dot(cartesian_coordinates, o2f_matrix.T) \n",
    "    frac_pos_incell = frac_pos % 1\n",
    "    for i in range(len(frac_pos_incell)):\n",
    "        if frac_pos_incell[i] < 0:\n",
    "            frac_pos_incell[i] += 1\n",
    "    if fractionalize:\n",
    "        return frac_pos_incell\n",
    "    else:\n",
    "        f2o_matrix = np.array(unit_cell.orthogonalization_matrix)\n",
    "        return np.dot(frac_pos_incell, f2o_matrix.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b629fa-4e70-4022-8841-6d83ff1085da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Remove Duplicate Blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36d02a4-ece2-4228-bf45-d7e7959aeaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_duplicates(blobs_df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Marks duplicate blobs in the DataFrame based on proximity in cartesian coordinates. \n",
    "    Checks on a per-sample basis, checking blobs with adjacent peak values.\n",
    "\n",
    "    Args:\n",
    "        blobs_df (pandas.DataFrame): The input DataFrame containing the blob information.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The modified DataFrame with an additional 'duplicate' column indicating duplicate blobs.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    blobs_df = blobs_df.sort_values(by='peak', ascending=False)\n",
    "    blobs_df['duplicate'] = 0  # Initialize 'duplicate' column with 0\n",
    "    \n",
    "    def check_euclidean_distance(list1, list2):\n",
    "        \n",
    "        \"\"\"\n",
    "        Checks if the distance between any pairwise points in two lists is less than 1.\n",
    "\n",
    "        Args:\n",
    "            list1 (list): The first list of cartesian coordinates.\n",
    "            list2 (list): The second list of cartesian coordinates.\n",
    "\n",
    "        Returns:\n",
    "            bool: Returns True if any distance is less than 1, otherwise returns False.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        for point1 in list1:\n",
    "            for point2 in list2:\n",
    "                distance = math.sqrt((point2[0] - point1[0])**2 + (point2[1] - point1[1])**2 + (point2[2] - point1[2])**2)\n",
    "                if distance < 1:\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    grouped = blobs_df.groupby('sample')\n",
    "    \n",
    "    for _, group in grouped:\n",
    "        if len(group) > 1:\n",
    "            all_possible_cart_lists = group['all_possible_cart'].tolist()\n",
    "            for i in range(1, len(all_possible_cart_lists)):\n",
    "                if check_euclidean_distance(all_possible_cart_lists[i-1], all_possible_cart_lists[i]):\n",
    "                    blobs_df.at[group.index[i], 'duplicate'] = 1\n",
    "    \n",
    "    return blobs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2323e869-9ec5-48f3-86f6-2f0985eaa748",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Tagging Blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d0a6a4-cc2a-4040-ad32-ba7c8f804f94",
   "metadata": {},
   "source": [
    "In this section, we tag and filter the blobs. We remove...\n",
    "\n",
    "- blobs that are duplicates (we occassionally have duplicate blobs due to an issue with gemmi's ASU mask)\n",
    "- blobs associated with the oxidation of `cys215`\n",
    "- blobs that belong to low quality samples (high r factors in refinement)\n",
    "- blobs that belong to samples with inconsistent data (in particular, Helen Ginn lists a few samples as hits without including a ligand in their bound state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b2d69-ab5f-4c68-a953-86f74ba1ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_df = pd.read_pickle(blob_folder + 'blob_stats.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f79b39-9398-4235-aea6-2eb06b759527",
   "metadata": {
    "tags": []
   },
   "source": [
    "Tag Samples that are Bound (1 if bound, 0 otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3488259-cb23-4d7a-8bf4-d10f8da48c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../bound_sample_ids.txt\") as f:\n",
    "    bound_samples = set([line.strip() for line in f])\n",
    "\n",
    "# Set the \"bound\" column based on whether or not each sample is in the bound samples list\n",
    "blob_df[\"bound\"] = blob_df[\"sample\"].apply(lambda x: 1 if x in bound_samples else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc644be0-423e-4514-a8ed-c449bd24812f",
   "metadata": {},
   "source": [
    "Tag Samples within 3A of a Cys215 Atom (1 if within, 0 otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bfa05c-337c-4aa6-a46a-a0b26c23ab8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob_df = tag_cys_215_blobs(blob_df, '../../pipeline/data/refined_models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41e2def-1c2b-4395-b448-3a2b12be3bd9",
   "metadata": {},
   "source": [
    "Tag Samples within `r` of a known LIG atom (1 if yes, 0 otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d027ce8-e558-43b5-a9af-c82fa10564ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "blob_df = tag_lig_blobs(blob_df, '../../pipeline/data/bound_models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf32b713-d46b-48d8-91b9-621c2f974d76",
   "metadata": {},
   "source": [
    "Tag Blobs that are Duplicates of Other Blobs (Patch for Gemmi's ASU Mask Issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eb28cf-33fb-4924-b4d3-7812dd75063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifies all possible cartesian coordinates after symmetry operations\n",
    "blob_df[['fractional', 'all_possible_frac', 'all_possible_cart']] = blob_df.apply(determine_locations, args=('../../pipeline/vae/reconstructed-phases/',), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2001ac-52f9-48f2-9f28-ad809d13f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marks blobs as duplicates if they are within 1A of another blob in the same sample\n",
    "blob_df = mark_duplicates(blob_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387ec19d-fd35-4c86-8dd7-2e13b9d6050f",
   "metadata": {},
   "source": [
    "Tag Blobs Belonging to Samples with High R Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c165e2-1a22-4560-940f-3bd596af5ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case, no blobs are removed\n",
    "\n",
    "r_factors = pd.read_csv('../../pipeline/data/refine_stats.csv')[['data_id', 'Rfree_final']]\n",
    "high_r_factors = r_factors.loc[r_factors['Rfree_final'] > 0.4, 'data_id'].astype(str).str.zfill(4)\n",
    "blob_df['high_r_factor'] = blob_df['sample'].isin(list(high_r_factors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c497905-2710-41ff-9262-371fa5847266",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_df.to_pickle('../../pipeline/vae/blobs/blob_stats_tagged.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf44bc-bb63-49d8-9e0e-21e9bf1d055f",
   "metadata": {},
   "source": [
    "Filter Blobs (Remove Cys215, High R Factor, Duplicates, and more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1dc04-ae37-43bd-a002-2eeeba6d225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_df = pd.read_pickle('../../pipeline/vae/blobs/blob_stats_tagged.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96327c3-f59a-4465-ac31-491c15182030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all samples where Helen Ginn does not include a bound state model\n",
    "# In this case, there are no blobs.\n",
    "\n",
    "hg_no_lig = ['0060', '1429', '1733', '1791', '0225', '0432', '0710']\n",
    "blob_df = blob_df[~blob_df['sample'].isin(hg_no_lig)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77bd816-629b-4342-a927-ebc7bc0d166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Cys215 related blobs, blobs in samples with high R factors, and duplicates\n",
    "\n",
    "blob_df = blob_df[(blob_df['cys215']==0) & (blob_df['high_r_factor']==0) & (blob_df['duplicate']==0)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8741a6e-8ee9-45ba-bac7-2fc55686be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_df.to_pickle('../../pipeline/vae/blobs/filtered_blob_stats_tagged.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d38d122-b7fa-4ef9-8a00-23da59c285eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Generate AUC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980e42a1-ce69-4815-b205-a324a3e5523d",
   "metadata": {},
   "source": [
    "In this section, we take the list of filtered blobs and 1) determine the AUC and 2) plot the ROC curve. We use `score` as the metric by which we classify blobs  a higher blob score means a higher likelihood that the blob represents ligand-binding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75577c49-4b83-42fc-966b-e55ce46ea26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_blob_stats(path, name=''):\n",
    "    \n",
    "    blob_df = pd.read_pickle(path)\n",
    "    \n",
    "    # create ROC curve\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(blob_df[\"ligand\"], blob_df[\"score\"], pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name=name)\n",
    "    display.plot()\n",
    "    \n",
    "    print(\"Total Number of Blobs:\", len(blob_df))\n",
    "    print(\"Total Number of Unique Samples:\", len(blob_df.drop_duplicates(subset='sample')))\n",
    "    \n",
    "    plt.savefig(os.path.dirname(path) + '/roc_curve.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1859f451-23a3-4ffa-a16e-9d8a50172f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_blob_stats('../../pipeline/vae/blobs/filtered_blob_stats_tagged.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e2d4b9-c0d7-44d9-941a-d9f2a459f5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_df = pd.read_pickle('../../pipeline/vae/blobs/filtered_blob_stats_tagged.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
