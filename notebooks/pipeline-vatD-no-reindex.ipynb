{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6a68262-e910-4e21-aaaf-1c74febbb0a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports & User-defined variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4824ce9-c211-4e77-9ccb-8da266a8c6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545740fa-2eeb-46e3-b7bf-395ec6ee30d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this may take 20-30 seconds\n",
    "import torch\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import shutil\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gemmi\n",
    "import reciprocalspaceship as rs\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from multiprocessing import Pool\n",
    "from itertools import repeat\n",
    "\n",
    "import valdo\n",
    "\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "plt.rc('figure', figsize=(4,2.5))\n",
    "print(\"done with imports.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca04ea52-52d3-4cd6-9cde-984566c97493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ncpu=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f6e18-0fb8-41ba-8c01-558585d91b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "bGPU, ncpu=valdo.helper.configure_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f366d283-2a7d-41b2-8a1a-b804e913833e",
   "metadata": {},
   "source": [
    "### Set user-defined variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa7480-58eb-45d6-bf37-79f03c2d3491",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed       = 11231        # random seed for PyTorch and subsampling data (if any) \n",
    "run_prefix        = \"run_6_iso_\" + str(random_seed) + \"_\"\n",
    "\n",
    "# filtering input to MTZ\n",
    "ref_mtz           = '058.mtz'    # use a high-quality dataset as reference\n",
    "mtzs_to_ignore    = []           # MTZs to disregard\n",
    "filter_by_Rfree   = True         # whether to filter out datasets for which refinement was poor (before scaling)\n",
    "max_R_factor      = 0.4          # Worst apo refinement Rfree-factor to keep. otherwise discarded.\n",
    "filter_by_cc      = True         # whether to filter out datasets fwith poor correlation with the reference (after scaling)\n",
    "min_cc_ref        = 0.4\n",
    "\n",
    "# VAE (most settings hardcoded below)\n",
    "fraction          = 1.0          # fraction of datasets to work with (default: 1.0)\n",
    "train_fraction    = 0.8          # fraction of working datasets used for training (sensible default: 0.8)\n",
    "latent_dim        = 7            # VAE latent space dimension (sensible default: 7)\n",
    "epochs            = 500          # Number of VAE training epochs (sensible default: 500)\n",
    "include_errors    = True         # whether to use SIGFs in the VAE loss function (sensible default: True)\n",
    "stdof             = 32         # DOF for student t (None -> normal dist); only used when include_errors=True\n",
    "eps               = 0.02         # the smaller eps (>=0), the more strongly estimated errors are taken into account (sensible default: 0.01)\n",
    "\n",
    "# difference maps\n",
    "ml_recon          = True        # Whether to use the ML reconstruction from the VAE rather than a sample (default: False)\n",
    "vae_samples       = 1            # number of samples to draw from the VAE during reconstruction (sensible default: 1), the variation is usually negligible\n",
    "w_pcts            = (90.0,99.99) # percentiles on sigDF and DelDF used in weights calc (sensible default: 95.00, 99.99)\n",
    "low_res_cutoff    = 1000.        # cutoff in A specifying what low-res reflections to keep (prob not helpful! sensible def: 1000.0)\n",
    "\n",
    "# blob finding\n",
    "blob_sig_cutoff   = 3.5          # cutoff for blob search (our default: 3.5)\n",
    "radius_in_A       = 4.0          # radius (in A) of the gaussian smoothing kernel; 3x the sigma (default: 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63960f66-3a1f-445f-8553-931157ad1144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the random number generator for reproducible behavior\n",
    "# change the random seed for another realization of the optimization.\n",
    "torch.manual_seed(random_seed)\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2010ec6f-a1b5-45d9-a70b-9fad5f2b6ff1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3303685-7006-4327-a960-eee685d21202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use trailing slashes!\n",
    "my_dir             = \"/n/holyscratch01/hekstra_lab/dhekstra/valdo-vatD/\" # directory where to keep all the processing steps\n",
    "original_data_path = \"/n/hekstra_lab/projects/valdo_vatD_FraserLab/VatD/vatd_mtzs/\" # the MTZs with measured F and sigF\n",
    "\n",
    "# keep the logs and pdb files in the same directory, e.g. with names xxxx.pdb, xxxx.mtz, and xxxx.log\n",
    "refinement_paths=[\"/n/hekstra_lab/projects/valdo_vatD_FraserLab/VatD/PHENIX_autorefinement/refine_output/short_names/\"\n",
    "                 ]\n",
    "refinement_phases = [(\"PH2FOFCWT\",\"PHFOFCWT\")\n",
    "                    ]\n",
    "# PHENIX phase options: *PHIF-model, PHFC, PH2FOFCWT, PHFOFCWT\n",
    "# dimple phase options: PHIC, PHIC_ALL, PHWT, *PHIC_ALL_LS\n",
    "\n",
    "# we may need a parser map our data MTZs to the refinement MTZs. \n",
    "# here are some examples:\n",
    "def apo_phases_parser_default(file): # for 0,2\n",
    "    return os.path.basename(file)\n",
    "\n",
    "def apo_phases_parser_4digit(file): # for 1\n",
    "    return os.path.basename(file)[0:4]+\".mtz\"\n",
    "    \n",
    "def apo_phases_parser_dimple(file): # for 3\n",
    "    return \"dimple_\"+os.path.basename(file)[0:4]+\"/final.mtz\"\n",
    "\n",
    "\n",
    "apo_phases_parser = apo_phases_parser_default\n",
    "\n",
    "which_phases = 0\n",
    "refined_path = refinement_paths[which_phases] # points to folder with refinement results; used for blob finding and indexing ambiguation\n",
    "phasing_path = refinement_paths[which_phases] # used for phasing of difference maps; could be the same as refined_path\n",
    "\n",
    "phase_2FOFC_col_in  = refinement_phases[which_phases][0]\n",
    "phase_FOFC_col_in   = refinement_phases[which_phases][1]\n",
    "phase_2FOFC_col_out = 'refine_PH2FOFCWT'\n",
    "phase_FOFC_col_out  = 'refine_PHFOFCWT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ccfb47-528c-4fa1-b63a-0c23bfcb9376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general pattern for standardizing mtz filenames\n",
    "mtz_file_pattern = r\".*(\\d{3}).*.mtz\"\n",
    "\n",
    "basepath = my_dir + 'pipeline/'\n",
    "data_path      = basepath + 'data/'\n",
    "input_mtz_path = basepath + 'data/mtzs_origin/'\n",
    "reindexed_path = basepath + 'data/mtzs_reindex/'\n",
    "scaled_path    = basepath + 'data/mtzs_scaled/'\n",
    "\n",
    "# These currently do not carry prefixes bc they will mostly be constant:\n",
    "intersection_path = scaled_path + 'intersection.pkl'\n",
    "union_path        = scaled_path + 'union.pkl'\n",
    "sigF_path         = scaled_path + 'sigF.pkl'\n",
    "\n",
    "vae_path = basepath + 'vae/'\n",
    "vae_reconstructed_path             = vae_path + 'reconstructed/'\n",
    "vae_reconstructed_with_phases_path = vae_path + 'reconstructed_w_phases/'\n",
    "blob_path                          = vae_path + 'blobs/'\n",
    "bound_models_standardized_path= basepath + 'data/bound_models_reindexed/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f812a13a-8f28-4be4-b0ec-c6cdd0c46525",
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitude_col        = 'FP' #'F-obs'\n",
    "amplitude_scaled_col = amplitude_col + '-scaled'\n",
    "error_col            = 'SIGFP' #'SIGF-obs'\n",
    "error_scaled_col     = error_col + '-scaled'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f2abc0-66fd-49bb-afed-aa6064b2fc1e",
   "metadata": {},
   "source": [
    "#### Configure paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde0b36a-5565-4a48-860f-9752aa10b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make paths\n",
    "dir_hierarchy=[basepath, data_path, input_mtz_path, reindexed_path, refined_path, scaled_path, vae_path, vae_reconstructed_path, \\\n",
    "               vae_reconstructed_with_phases_path, blob_path, bound_models_standardized_path]\n",
    "for folder in dir_hierarchy:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "        print(f\"{folder:<80}\" + f\"{'created': >20}\")\n",
    "    else:\n",
    "        print(f\"{folder:<80}\" + f\"{'already exists': >20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e101699-3d69-4bca-9033-1736845da4f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1: Diffraction Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabe69ce-7272-4802-993c-293a08402662",
   "metadata": {},
   "source": [
    "The first step involves acquiring diffraction datasets in the `mtz` format. These datasets should follow a specific naming convention, where each file is named with a number followed by the `.mtz` extension (e.g., `0001.mtz`, `0002.mtz`, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb296c84-c04a-460f-b2fe-182c200a10e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03092f4d-aac8-40f3-87e2-d9c0ced26fc3",
   "metadata": {},
   "source": [
    "1. Ensure that you have collected diffraction datasets in the `mtz` format.\n",
    "\n",
    "2. Organize the datasets with sequential numerical names (e.g., `0001.mtz`, `0002.mtz`, etc.). You can do so using the following cell.\n",
    "\n",
    "3. If you have bound-state models already and you want to benchmark against those in Step 9 below, also run the cell `Application to bound state models (for Step 9)`. \n",
    "\n",
    "Following this naming convention will allow datasets to be ready for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4333b1cb-c634-41ba-8cc7-36e5dac8d2b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Copy and Renaming Code\n",
    "_Only need to run the following cell once, can skip on rerun._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a176fe-9487-4818-9a35-d204fe629da1",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following cell is a template for renaming files to the correct naming convention. Change `source_path`, `destination_path`, and extensions as necessary.\n",
    "\n",
    "e.g. `source_path/PTP1B-y0798_mrflagsref_idxs.mtz` -> `destination_path/0798.mtz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2574860-1cb6-4935-b0d9-1433a20e8852",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new_mtzs=valdo.helper.standardize_input_mtzs(source_path=original_data_path, \n",
    "                       destination_path=input_mtz_path, \n",
    "                       mtz_file_pattern=mtz_file_pattern, \n",
    "                       ncpu=ncpu)\n",
    "print(\"\\nCreated \" + str(len(new_mtzs)) + \" standardized MTZ files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd88a92-9666-43e3-a1dc-e685bba103a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 2: Reindexing, Automatic Refinement & Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039e036e-c6c0-4283-a5f6-eef58a8dd678",
   "metadata": {},
   "source": [
    "This step focuses on automatic refinement and scaling a list of input MTZ files to a reference MTZ file using gemmi. If you have already performed automated refinement against each dataset, you can skip it here and just take this as information. The VatD data do not require reindexing. For an example that does require reindexing, see the PTP1B notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c75902-c9d3-4858-9793-1281e78348f7",
   "metadata": {},
   "source": [
    "**Scaling**: the present example then goes on to scaling in order to make sure the data have similar magnitude and resolution dependence before being fed into the VAE.\n",
    "If only intensities (and not amplitudes) are available, you can perform French-Wilson scaling here too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8bb40e-7255-4066-9b08-a21e293c2211",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08a5d0-92aa-4f96-8e03-2476910481b6",
   "metadata": {},
   "source": [
    "The following cells do the following (and you'd need to do the same in a command script):\n",
    "\n",
    "1. Import the required library, `valdo`.\n",
    "\n",
    "2. If necessary, perform French-Wilson scaling (we recommend generating amplitudes using your data reduction software!).\n",
    "\n",
    "3. Create a `Scaler` object by providing the path to the reference MTZ file.\n",
    "\n",
    "4. Call the `batch_scaling()` method of the `Scaler` object. The `batch_scaling()` method will apply the scaling process to each input MTZ file and save the scaled MTZ files in the specified output folder. Scaling metrics, such as least squares values and correlations, will be saved in the report file.\n",
    "\n",
    "    This function can be called with the following parameters:\n",
    "    - `mtz_path_list`: List of paths to input MTZ files to be scaled.\n",
    "    - `outputmtz_path`: Path to the folder where the scaled MTZ files will be saved (optional, default is `./scaled_mtzs/`).\n",
    "    - `reportfile`: Path to the file where scaling metrics will be saved (optional, default is `./scaling_data.json`).\n",
    "    - `verbose`: Whether to display verbose information during scaling (optional, default is `True`).\n",
    "    - `n_iter`: Number of iterations for the analytical scaling method (optional, default is `5`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f13e11-7b4d-4845-84b0-b59451f6f3db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### French-Wilson scaling (_if needed_) \n",
    "_Only need to run the following once, so can skip on rerun._\n",
    "\n",
    "_Only run the two cells below if the data reduction software did not calculate suitable F, SigF._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de3e185-789c-4768-adfd-25632d3baeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of files to be reindexed\n",
    "file_list = glob.glob(input_mtz_path + \"*mtz\")\n",
    "file_list.sort()\n",
    "\n",
    "# Drop mtzs in the ignore list\n",
    "for mtz in mtzs_to_ignore:\n",
    "    file_list.remove(input_mtz_path+mtz)\n",
    "print(\"Working with \" + str(len(file_list)) + \" MTZ files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbb991b-585d-443a-b240-b92f7508a8e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# if this does not complete within 5 min, please lower tmp_cpu below ncpu and try again.\n",
    "tmp_cpu=12\n",
    "tmp=valdo.helper.apply_french_wilson(file_list, \\\n",
    "                                     intensity_key=\"IMEAN\", \\\n",
    "                                     sigma_key=\"SIGIMEAN\", \\\n",
    "                                     output_columns=[\"IP\",\"SIGIP\",\"FP\",\"SIGFP\"], \n",
    "                                     isotropic=True, \\\n",
    "                                     ncpu=tmp_cpu\n",
    "                                    )\n",
    "print(f\"Performed French-Wilson scaling for {np.sum(tmp)} out of {len(file_list)} input MTZ files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4147fd29-496d-4bdb-90c1-f0bf8e2de298",
   "metadata": {},
   "source": [
    "Select a good dataset as your reference (see the variable `ref_mtz` above).\n",
    "\n",
    "**Critical**: The phases we will use below to calculate difference maps must come from models that have been indexed _consistent_ with the indexing convention chosen here (by the choice of our reference dataset). Otherwise, the difference maps will be meaningless!! An easy way to assure consistency is by obtaining the phases from refinement against the reindexed data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996622f0-1b8e-4ead-ae4e-0e14b8068454",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Automatic Refinement Code Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5ae585-1633-465f-89ee-9ab468ec9030",
   "metadata": {},
   "source": [
    "Automatic refinement should happen outside the notebook. **Note:** It is recommended to run the automatic refinement in a parallel manner for many datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d2dda4-8c68-4b62-8231-a79d875303d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example command to run automatic refinement:\")\n",
    "print(\" \")\n",
    "print(f\"valdo.refine --pdbpath 'xxx/xxx_apo.pdb' --mtzpath '{input_mtz_path}*.mtz' --output '{refined_path}' --eff 'xxx/refine_drug.eff'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af78630-7034-4985-9095-0f7dbfa07a40",
   "metadata": {},
   "source": [
    "### After Automatic refinement\n",
    "After completing the refinement process, all outcomes will be located in the `refined_path` (which you can set in the first section of this notebook).\n",
    "If you are utilizing PHENIX for refinement, you can generate refinement statistics using the following code. We will look at this because it may (or may not) be a good idea to filter your datasets based on how well they refine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e15ee6e-7bdf-4cf8-a27c-451dbcc660cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loglist = glob.glob(os.path.join(refined_path, \"*log\"))\n",
    "loglist.sort()\n",
    "\n",
    "idx = []\n",
    "Rw_start = []\n",
    "Rf_start = []\n",
    "Rw_final = []\n",
    "Rf_final = []\n",
    "time = []\n",
    "\n",
    "print(os.path.basename(loglist[0]))\n",
    "print(os.path.basename(loglist[0]).replace('.','_').split(\"_\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2d259-70ca-4f86-ac9d-188f0f65d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in tqdm(loglist):\n",
    "    with open(file, 'r') as f:\n",
    "        log = f.readlines()\n",
    "\n",
    "    # may need to modify the parsing of the filenames depending on your naming conventions:\n",
    "    # print(os.path.basename(file).split(\"_\")) \n",
    "    idx_temp, _  = os.path.basename(file).replace('.','_').split(\"_\")\n",
    "    try:\n",
    "        Rw_final_temp, Rf_final_temp = float(log[-2].replace(\",\",\" \").split(\" \")[3]), float(log[-2].replace(\",\",\" \").split(\" \")[-1]) # Final\n",
    "        Rw_start_temp, Rf_start_temp = float(log[-3].replace(\",\",\" \").split(\" \")[3]), float(log[-3].replace(\",\",\" \").split(\" \")[-1]) # Start\n",
    "        time_temp = float(log[-1].strip().split(\" \")[-2]) # Time\n",
    "    except Exception as e:\n",
    "        print(\"Sample \", idx_temp, \" refinement fails!\")\n",
    "        print(e)\n",
    "        Rw_final_temp, Rf_final_temp = float(\"nan\"), float(\"nan\")\n",
    "        Rw_start_temp, Rf_start_temp = float(\"nan\"), float(\"nan\")\n",
    "        time_temp = float(\"nan\")\n",
    "    idx.append(idx_temp)\n",
    "    Rw_start.append(Rw_start_temp)\n",
    "    Rf_start.append(Rf_start_temp)\n",
    "    Rw_final.append(Rw_final_temp)\n",
    "    Rf_final.append(Rf_final_temp)\n",
    "    time.append(time_temp)\n",
    "\n",
    "df_refine = pd.DataFrame({\n",
    "    \"file_idx\"   : idx,\n",
    "    \"Rw_start\"   : Rw_start,\n",
    "    \"Rf_start\"   : Rf_start,\n",
    "    \"Rw_final\"   : Rw_final,\n",
    "    \"Rf_final\"   : Rf_final,\n",
    "    \"time(s)\"    : time\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a78af95-82d9-49d1-b0cc-25229a8d0d85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_refine.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4568856a-8b2c-4b0a-af9a-5c8e6bddc079",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_badR = df_refine[df_refine[\"Rf_final\"] > max_R_factor].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb696b0-3e5d-49bc-9afa-33f4b0fe8977",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_list=[]\n",
    "if filter_by_Rfree:\n",
    "    # just keeping track. not yet applied!\n",
    "    filter_list = (df_badR[\"file_idx\"] + '.mtz').tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d117a7e-357a-4008-aed9-5e2fa2129726",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scaling Code\n",
    "_Only need to run the following cell once, can skip on rerun._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0d95fe-1efa-4f7d-81b8-aeed41c38372",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(input_mtz_path + \"*mtz\")\n",
    "file_list.sort()\n",
    "\n",
    "for mtz in filter_list:\n",
    "    try:\n",
    "        file_list.remove(input_mtz_path+mtz)\n",
    "    except:\n",
    "        print(mtz + \" already not in list\")\n",
    "    \n",
    "print(\"We have \" + str(len(file_list)) + \" MTZ files ready for scaling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c7e28a-b2f4-4f08-bf6d-25d6276f91f0",
   "metadata": {},
   "source": [
    "The following optimization of scales sometimes generates numerical warnings about overflows. Let us know if this happens a lot. Without parallelization, this takes about 0.25s/dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf37c48-5012-4306-b690-7b9b9f74f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_idx = file_list.index(input_mtz_path+ref_mtz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cfd4a7-752a-4967-8c05-39357a3a4cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaled_path)\n",
    "# print(glob.glob(scaled_path + \"*mtz\"))\n",
    "prior_scaled_mtzs=glob.glob(scaled_path + \"*mtz\")\n",
    "print(f\"Contained \" +str(len(prior_scaled_mtzs)) + \" files.\")\n",
    "print(\"Removing...\")\n",
    "for f in prior_scaled_mtzs:\n",
    "    os.remove(f)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d989673-1405-4abe-b168-7fbdda97c406",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Scales all datasets to the previously provided reference, writes a `metrics.pkl`, take ~30s\n",
    "# Returns some metrics on the quality of the dataset.\n",
    "\n",
    "if ncpu > 1:\n",
    "    scaler = valdo.Scaler_pool(reference_mtz=file_list[reference_idx],columns=[amplitude_col, error_col],verbose=False, n_iter=5,ncpu=ncpu)\n",
    "    scaling_metrics = scaler.batch_scaling(mtz_path_list=file_list, \n",
    "                                   outputmtz_path=scaled_path,\n",
    "                                   prefix=run_prefix)\n",
    "else:\n",
    "    scaler = valdo.Scaler(reference_mtz=file_list[reference_idx])\n",
    "    scaling_metrics = scaler.batch_scaling(mtz_path_list=file_list, \n",
    "                                   outputmtz_path=scaled_path,\n",
    "                                   prefix=run_prefix, \n",
    "                                   verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8d1f2f-b9ba-463a-8811-2015cbe59794",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_metrics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161d6643-f735-4eb1-9f63-382ffa3e8912",
   "metadata": {},
   "source": [
    "- Most of the datasets for which the LS goes up also have bad Rfree.\n",
    "- Keep in mind that LS scales with number of reflections. A better comparison would normalize for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fff5d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scaling_metrics[\"start_LS\"].to_numpy(),scaling_metrics[\"end_LS\"].to_numpy(),'.',alpha=0.5)\n",
    "xlim=plt.xlim()\n",
    "plt.plot(xlim,xlim,'r-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Starting LS\")\n",
    "plt.ylabel(\"Final LS\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.hist(scaling_metrics['end_corr'],20)\n",
    "plt.xlabel('Correlation to reference')\n",
    "plt.ylabel('Count')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c24816-8832-4a38-953e-e0e02df2b4ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3: Normalization & Preparation of VAE input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4c5cf6-11a1-45f1-bc60-cb64aa6b7b06",
   "metadata": {},
   "source": [
    "This step involves normalizing the scaled structure factor amplitudes obtained in the previous step. The input is restricted to only those Miller indices present in the _intersection_ of all datasets, and the VAE will predicts structure factor amplitudes for all Miller indices in the _union_ of all datasets.\n",
    "\n",
    "Additionally, we standardize all the input data, such that the structure factor amplitudes for each Miller index in the union of all datasets have a mean of zero and a unit variance across datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e23f578-3067-43a8-a3b8-c78fbcdb4399",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9769802-6333-4d7b-adab-37a54da31847",
   "metadata": {},
   "source": [
    "1. Import the required library, `valdo.preprocessing`.\n",
    "\n",
    "2. Find the intersection and union of the scaled datasets using the following functions:\n",
    "\n",
    "   - `find_intersection()`: Finds the intersection of `amplitude_col` from multiple input MTZ files and saves the result to the specified output pickle file. Arguments include the following:\n",
    "\n",
    "      - `input_files`: List of input MTZ file paths.\n",
    "      - `output_path`: Path to save the output pickle file containing the intersection data.\n",
    "      - `amplitude_col`: Name of the column in the dataset that represents the scaled amplitude (default is 'F-obs-scaled').\n",
    "\n",
    "   - `find_union()`: Finds the union of `amplitude_col` from multiple input MTZ files and saves the result to the specified output pickle file. Arguments are the same as `find_intersection()`.\n",
    "\n",
    "3. Generate the VAE input and output data using the `generate_vae_io()` function. This standardizes the intersection dataset using mean and standard deviation calculated from the union dataset. The standardized intersection becomes the VAE input, while the standardized union becomes the VAE output. Both the VAE input and output are saved to the specified folder. \n",
    "\n",
    "    This function can be called with the following parameters:\n",
    "\n",
    "    - `intersection_path`: Path to the intersection dataset pickle file.\n",
    "    - `union_path`: Path to the union dataset pickle file.\n",
    "    - `io_path`: Path to the output folder where the VAE input and output will be saved. Mean and standard deviation data calculated from the union dataset will also be saved in this folder as `union_mean.pkl` and `union_sd.pkl`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deafcf0-7551-4c81-b9cc-dfdc7f66c3dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Filtering Samples Code\n",
    "\n",
    "_Only need to run this code once, can skip on rerun._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bdb137-e08c-48bd-8166-a577f5b12c05",
   "metadata": {},
   "source": [
    "In this example, we remove samples with low `end_corr`. This ensures that our VAE is trained with high quality samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dbb815-398f-4797-a191-ea9ed6a8bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all scaled files to use as input and output for the VAE\n",
    "\n",
    "file_list = glob.glob(scaled_path + \"*mtz\")\n",
    "file_list.sort()\n",
    "print(\"Current file list contains \" + str(len(file_list)) + \" entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ab1fd-9e96-4dcd-9572-010c5022df9a",
   "metadata": {
    "tags": []
   },
   "source": [
    "This following cell removes samples with `end_corr < min_cc_ref` or if `end_corr = NA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ad07d-e684-4aed-b568-9117ab062cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.read_pickle(scaled_path + run_prefix + \"scaling_metrics.pkl\")\n",
    "# metrics_df[metrics_df.isna().any(axis=1)]\n",
    "\n",
    "if filter_by_cc:\n",
    "    low_corr_files = list(metrics_df[ (metrics_df['end_corr'] < min_cc_ref) | \\\n",
    "                                      (metrics_df['end_corr'].isnull())]['file'])\n",
    "    low_corr_files = [scaled_path + x + '.mtz' for x in low_corr_files]\n",
    "    file_list = [file for file in file_list if file not in low_corr_files]\n",
    "\n",
    "with open(os.path.join(vae_path, 'filtered_file_list.pkl'), 'wb') as f:\n",
    "    pickle.dump(file_list, f)\n",
    "print(\"The file list contains \" + str(len(file_list)) + \" entries after filtering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412569ad-4787-4f7d-a2a1-4af64e49e999",
   "metadata": {},
   "source": [
    "##### For testing purposes, we will subsample the list of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2cb462-46ef-43b1-b7a1-fe2df24780e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fraction < 1.0:\n",
    "    file_list = sample(file_list, int(fraction * len(file_list)))\n",
    "print(\"Subsampling \" + str(len(file_list)) + \" entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbb01bb-9db7-476b-93f8-e943cb503edd",
   "metadata": {},
   "source": [
    "### Generating VAE Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94831fcd-f842-4c37-96ec-3e750a245b45",
   "metadata": {},
   "source": [
    "The following cells generate the VAE input and output. These cells do take seconds to minutes to run (depending on `ncpu`)\n",
    "\n",
    "**Please be patient!**\n",
    "\n",
    "If you are not changing anything about scaling or dataset filtering, you can skip them next time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64475a6-ae79-4136-81da-d955fe548e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Creates an `intersection.pkl` file at the specified path\n",
    "# This is the intersection of all the scaled files provided\n",
    "valdo.preprocessing.find_intersection(input_files=file_list, \n",
    "                  output_path=intersection_path,\n",
    "                  amplitude_col=amplitude_scaled_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ec5d5a-d74f-4730-ad5c-8a25b4823fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Creates a `union.mtz` file at the specified path\n",
    "# This is the union of all the scaled files provided\n",
    "# This is a lot slower than the intersection step about. The step is \n",
    "#   about linear in the number of files, 0.5 sec/file on my CPU.\n",
    "# Inclusion of errors does not affect speed -- *always include them*\n",
    "\n",
    "valdo.preprocessing.find_union(input_files=file_list, \n",
    "                               output_path=union_path,\n",
    "                               sigF_path=sigF_path,\n",
    "                               amplitude_col=amplitude_scaled_col, \n",
    "                               error_col=error_scaled_col,\n",
    "                               include_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32f2c8d-3d30-414b-9c68-c194dbc9b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Generates VAE input and output data from the intersection and union datasets\n",
    "# Inclusion of errors does not affect speed => always include them here.\n",
    "# Note that we do not currently prefix the VAE input\n",
    "valdo.preprocessing.generate_vae_io(intersection_path=intersection_path, \n",
    "                                    union_path=union_path, \n",
    "                                    sigF_path=sigF_path,\n",
    "                                    io_folder=vae_path, \n",
    "                                    prefix=\"\", \n",
    "                                    include_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20af9fe-70d3-4683-b199-c4ee0d8b946f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 4: VAE Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bbe4f6-6158-403c-855e-a9ae78a9a00b",
   "metadata": {},
   "source": [
    "In this step, we train the VAE model using the provided VAE class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c3057-4c5c-43fd-94bc-fc718affe32e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a414ee3a-0bc9-4344-8ab4-1090ac02eb68",
   "metadata": {},
   "source": [
    "1. Load the VAE input and output data that was generated in the previous step.\n",
    "\n",
    "2. Initialize the VAE model with the desired hyperparameters. Tune-able hyperparameters include the following:\n",
    "    - `n_dim_latent`: Number of dimensionality in latent space (optional, default `1`)\n",
    "\n",
    "    - `n_hidden_layers`: Number of hidden layers in the encoder and decoder. If an int is given, it will applied to both encoder and decoder; If a length 2 list is given, first int will be used for encoder, the second will be used for decoder\n",
    "\n",
    "    - `n_hidden_size`: Number of units in hidden layers. If an int is given, it will be applied to all hidden layers in both encoder and decoder; otherwise, an array with length equal to the number of hidden layers can be given, the number of units will be assigned accordingly.\n",
    "\n",
    "    - `activation` : Activation function for the hidden layers (optional, default `tanh`)\n",
    "\n",
    "3. Split the data into training and validation sets. Randomly select a subset of indices for training and use the rest for validation.\n",
    "\n",
    "4. Convert the data into PyTorch tensors.\n",
    "\n",
    "5. Set up the optimizer for training.\n",
    "\n",
    "6. Train the VAE model using the `train()` method. The training process involves minimizing the ELBO (Evidence Lower Bound) loss function, which consists of a Negative Log-Likelihood (NLL) term and a Kullback-Leibler (KL) divergence term. Arguments used in this function include:\n",
    "\n",
    "    - `x_train`: Input data for training the VAE, a PyTorch tensor representing the VAE input data. \n",
    "\n",
    "    - `y_train`: Output data for training the VAE, a PyTorch tensor representing the VAE output data. \n",
    "\n",
    "    - `optim`: The optimizer used for training the VAE, a PyTorch optimizer object, such as `torch.optim.Adam`, that specifies the optimization algorithm and its hyperparameters, including the learning rate (`lr`).\n",
    "\n",
    "    - `x_val`: Input data for validation during training. (optional, default is `None`).\n",
    "\n",
    "    - `y_val`: Output data for validation during training. (optional, default is `None`).\n",
    "\n",
    "    - `epochs`: The number of training epochs (epoch: a single pass through the data).\n",
    "\n",
    "    - `batch_size`: The batch size used during training. If an integer is provided, the same batch size will be used for all epochs. If a list of integers is provided, it should have the same length as the number of epochs, and each value in the list will be used as the batch size for the corresponding epoch. Default is `256`.\n",
    "\n",
    "    - `w_kl`: The weight of the Kullback-Leibler (KL) divergence term in the ELBO loss function. The KL divergence term encourages the latent distribution to be close to a prior distribution (usually a standard normal distribution). A higher value of `w_kl` will increase the regularization strength on the latent space. Default is `1.0`.\n",
    "    - `include_errors`, `eps`, `stdof`: when including errors, the loss will be weighted by the errors in the observed amplitudes. A small fixed amount, `eps` can be added to these error estimates to diminish the contribution of the reflections with very small sigF (e.g. around 0.005 after normalization). `stdof` toggles the loss function between Student t (finite `stdof`) and Normal (`stdof=None`).\n",
    "\n",
    "    **Note:** The VAE class internally keeps track of the training loss (`loss_train`) and its components (NLL and KL divergence) during each batch of training. These values can be accessed after training to monitor the training progress and performance. The `loss_train` attribute of the VAE object will be a list containing the training loss values for each batch during training. The `loss_names` attribute contains the names of the loss components: \"Loss\", \"NLL\", and \"KL_div\". These attributes are updated during training and can be used for analysis or visualization.\n",
    "\n",
    "7. Save the trained VAE model for future use (optional)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd901df-c30b-4d73-95a3-79e18491136e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f672b-47ce-464c-997b-3ed9ececd1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VAE I/O Files Generated\n",
    "# note that we do not use a prefix since these files are likely to change less frequently than the runs.\n",
    "vae_input  = np.load(vae_path + 'vae_input.npy')\n",
    "vae_output = np.load(vae_path + 'vae_output.npy')\n",
    "vae_sigF   = np.load(vae_path + 'vae_sigF.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c70c992-6ff2-4890-8330-09384cba57f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "print(vae_input.shape)\n",
    "print(vae_output.shape)\n",
    "print(vae_sigF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d667f-30a3-468d-91ac-c3067d898223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to see the training target in all its glory...\n",
    "# union = pd.read_pickle(union_path)\n",
    "# union.iloc[:5,:20]\n",
    "\n",
    "# or the (normalized) sigF\n",
    "# sigF = pd.read_pickle(sigF_path)\n",
    "# sigF.iloc[:5,:20]\n",
    "\n",
    "# SNR_ratio = sigF.iloc[:500,:500].values.flatten()/union.iloc[:500,:500].values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29715979-2e73-4a58-87d5-d60cf33b2d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(vae_output[10,:],vae_sigF[10,:],'.',alpha=0.01)\n",
    "plt.xlabel(\"Normalized input amplitudes (can be negative!)\")\n",
    "plt.ylabel(\"Normalized SigF\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807935b6-0120-4494-a5fb-02faf1da4a86",
   "metadata": {},
   "source": [
    "#### Setting up the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd945d-04b7-4c46-b320-a236bad7b606",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Specify VAE Parameters; takes ~1 sec\n",
    "# Only execute once. Otherwise it may throw an error.Restart kernel if this cell throws an error\n",
    "latent_dimension = latent_dim\n",
    "train_fraction = train_fraction\n",
    "epochs = epochs # default: 300\n",
    "\n",
    "vae = valdo.VAE(n_dim_i = vae_input.shape[1], \n",
    "      n_dim_o = vae_output.shape[1], \n",
    "      n_dim_latent = latent_dimension, \n",
    "      n_hidden_layers = [3, 6], \n",
    "      n_hidden_size = 100, \n",
    "      activation = torch.relu)\n",
    "\n",
    "# Randomly select (train_fraction) indices for training\n",
    "choice = np.random.choice(vae_input.shape[0], int(train_fraction*vae_input.shape[0]), replace=False)    \n",
    "train_ind = np.zeros(vae_input.shape[0], dtype=bool)\n",
    "train_ind[choice] = True\n",
    "test_ind = ~train_ind\n",
    "print(\"Size of training set = \" + str(np.sum(train_ind)))\n",
    "print(\"Size of test set = \" + str(np.sum(test_ind)))\n",
    "\n",
    "# Split the input and output data into training and validation sets\n",
    "x_train, x_val = vae_input[train_ind], vae_input[test_ind]\n",
    "y_train, y_val = vae_output[train_ind], vae_output[test_ind]\n",
    "e_train, e_val = vae_sigF[train_ind], vae_sigF[test_ind] # error estimates for y\n",
    "\n",
    "# Convert the data to torch tensors\n",
    "x_train, x_val, y_train, y_val, e_train, e_val = torch.tensor(x_train), torch.tensor(x_val), \\\n",
    "                                                 torch.tensor(y_train), torch.tensor(y_val), \\\n",
    "                                                 torch.tensor(e_train), torch.tensor(e_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d57a324-ec20-47bd-ba2c-1bc12b6d7766",
   "metadata": {},
   "source": [
    "#### Training the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8754446-c53c-4085-b5ea-5683f3843ad2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Set up the optimizer and train the VAE\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "vae.train(x_train, y_train, e_train, optimizer, x_val, y_val, e_val, epochs=epochs, \n",
    "          batch_size=100, w_kl=1.0,\n",
    "          eps=eps,\n",
    "          include_errors=include_errors,\n",
    "          stdof=stdof, \n",
    "          verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8ce422-7576-4178-9192-be428994be20",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the trained VAE model\n",
    "vae.save(vae_path + run_prefix + 'trained_vae.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb40c5db-0667-4599-a72f-7ee75fbd199c",
   "metadata": {},
   "source": [
    "#### VAE loss traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5261640d-d60b-44bd-86df-892d2a61bd19",
   "metadata": {},
   "source": [
    "The following cells allow us to visualize the loss over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8150f5d8-8b5a-403e-a7df-687028e93817",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = valdo.VAE.load(vae_path + run_prefix + 'trained_vae.pkl')\n",
    "loss_array = np.array(vae.loss_train)\n",
    "plt.figure(figsize=(5,3.5))\n",
    "plt.plot(loss_array)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss term\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ec823-ee45-4261-9239-aced93af3d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=3, ncols=1, figsize=[6,8])\n",
    "ax = axs.reshape(-1)\n",
    "\n",
    "ax[0].plot(loss_array[:,0], label='Total Loss, Training')\n",
    "ax[0].plot(loss_array[:,3], label='Total Loss, Validation')\n",
    "ax[0].set_xlabel(\"Steps\")\n",
    "ax[0].grid()\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(loss_array[:,1], label='Negative Log Likelihood, Training')\n",
    "ax[1].plot(loss_array[:,4], label='Negative Log Likelihood, Validation')\n",
    "ax[1].set_xlabel(\"Steps\")\n",
    "ax[1].grid()\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(loss_array[:,2], label='KL Divergence, Training')\n",
    "ax[2].plot(loss_array[:,5], label='KL Divergence, Validation')\n",
    "ax[2].set_xlabel(\"Steps\")\n",
    "ax[2].grid()\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ea7b78-bb2d-4709-af00-44b3901539ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Steps 5 & 6: Reconstruction of \"Apo\" Data & Calculating Difference Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62471f2c-4673-4ab1-b3a9-231ac0c93849",
   "metadata": {},
   "source": [
    "In this step, VAE outputs are re-scaled accordingly to recover the original scale, and differences in amplitudes between the original and reconstructed data are calculated. A `recons` and a `diff` column will be created for all datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de80754-e5c3-47d2-a4f0-32c344cbc9f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb463176-561c-4c21-9934-5928074d1be3",
   "metadata": {},
   "source": [
    "To perform the reconstruction, or re-scaling, the `rescale()` function can be called, providing the necessary arguments:\n",
    "\n",
    "- `recons_path`: Path to the reconstructed output of the VAE in NumPy format.\n",
    "- `intersection_path`: Path to the pickle file containing the intersection of all scaled datasets.\n",
    "- `union_path`: Path to the pickle file containing the union data of all scaled datasets.\n",
    "- `input_files`: List of input file paths. This list should be in the same order as is in the `vae_input.npy` or `intersection.mtz`.\n",
    "- `info_path`: Path to the folder containing files with the mean and SD used for standardization previously.\n",
    "- `output_path`: Path to the folder where the reconstructed data will be saved.\n",
    "- `amplitude_col`: Column in the MTZ file that contains structure factor amplitudes to calculate the difference column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b453bd09-674b-40d2-9d29-3c47a1397014",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Code\n",
    "_Loads its inputs, saves its outputs._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a68d24-8676-4432-9bbb-ecd773f382cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained VAE\n",
    "vae = valdo.VAE.load(vae_path + run_prefix + 'trained_vae.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b5ae3e-b02b-4ade-807a-1a9852fa080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input file and create a tensor\n",
    "\n",
    "vae_input = np.load(vae_path + 'vae_input.npy')\n",
    "vae_input_tensor = torch.tensor(vae_input)\n",
    "if bGPU:\n",
    "    vae_input_tensor = vae_input_tensor.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e34fa3-7d56-4061-acaa-772d1577c7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the input file via VAE, convert to numpy, and save\n",
    "recons = vae.reconstruct(vae_input_tensor, ml_recon=ml_recon, repeats=vae_samples)\n",
    "if vae_samples > 1:\n",
    "    recons_list = []\n",
    "    for item in recons:\n",
    "        recons_list.append(item.detach().cpu().numpy())\n",
    "    recons_3d = np.array(recons_list)\n",
    "    recons=np.array([np.mean(recons_3d,axis=0), np.std(recons_3d,axis=0)])\n",
    "else:\n",
    "    recons = recons.detach().cpu().numpy()\n",
    "    \n",
    "np.save(vae_reconstructed_path + 'recons', recons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffe9dd2-3e6c-41bd-86b3-9fecb3c0e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "if vae_samples > 1:\n",
    "    plt.hist(recons[1].flatten(),20)\n",
    "    plt.yscale('log')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622dd5cf-b8b6-4b4f-a627-9992090d2f06",
   "metadata": {},
   "source": [
    "In the following step we put the reconstructed amplitudes back on the same scale as the input data. This step can crash the kernel/session if not enough memory is available. In that case, try reducing ncpu below the available number of CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c5ef8-1c50-49f2-9fac-b093d4be808b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Re-scale the reconstructed files accordingly and creates the `diff` column\n",
    "# Function is valdo.preprocessing.rescale\n",
    "\n",
    "ncpu_temp=10#ncpu/2\n",
    "\n",
    "with open(os.path.join(vae_path, 'filtered_file_list.pkl'),'rb') as f:\n",
    "    file_list = pickle.load(f)\n",
    "\n",
    "print(len(file_list))\n",
    "\n",
    "if ncpu > 1:\n",
    "    valdo.preprocessing.rescale_pool(recons_path=vae_reconstructed_path + 'recons.npy', \n",
    "                intersection_path=intersection_path, \n",
    "                union_path=union_path, \n",
    "                input_files=file_list, \n",
    "                info_folder=vae_path, \n",
    "                output_folder=vae_reconstructed_path,\n",
    "                amplitude_col=amplitude_scaled_col,\n",
    "                ncpu=ncpu_temp)\n",
    "else:\n",
    "    valdo.preprocessing.rescale(recons_path=vae_reconstructed_path + 'recons.npy', \n",
    "                intersection_path=intersection_path, \n",
    "                union_path=union_path, \n",
    "                input_files=file_list, \n",
    "                info_folder=vae_path, \n",
    "                output_folder=vae_reconstructed_path,\n",
    "                amplitude_col=amplitude_scaled_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e299c3d3-2425-4900-8536-397bd2cf1d2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Steps 7 & 8: Gaussian Blurring & Searching for Blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a39437b-916b-4b8c-8392-07cefffe6734",
   "metadata": {},
   "source": [
    "In this step, we aim to identify significant changes in electron density caused by ligand binding to a protein. By taking the absolute value of the electron density difference maps and applying Gaussian blurring, a new map is created with merged positive electron density blobs. The blurring process attempts to reduce noise. Blobs are then identified and characterized above a specified contour level and volume threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7343d894-1c13-4dbf-8dbd-38a3880852ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e243ac5-1ca2-4a7b-9ccb-d9057de96617",
   "metadata": {},
   "source": [
    "To generate blobs from electron density maps, call the `generate_blobs()` function, which takes electron density map files and corresponding refined protein models as inputs. The function preprocesses the maps and identifies blobs above a specified contour level and volume threshold (the volume threshold is the default set by `gemmi`). The output is a DataFrame containing statistics for each identified blob, including peak value, score, centroid coordinates, volume, and radius. \n",
    "\n",
    "This function can be called with the following arguments:\n",
    "\n",
    "- `input_files`: List of input file paths.\n",
    "- `model_path`: Path to the folder containing the refined models for each dataset (pdb format).\n",
    "- `diff_col`: Name of the column representing diffraction values in the input MTZ files.\n",
    "- `phase_col`: Name of the column representing phase values in the input MTZ files.\n",
    "- `output_path`: Path to the output folder where the blob statistics DataFrame will be saved.\n",
    "- `cutoff`: Blob cutoff value. Blobs with values below this cutoff will be ignored (optional, default is `5`).\n",
    "- `negate`: Whether to negate the blob statistics (optional, default is `False`). Use True if there is interest in both positive and negative peaks, which is not typically of interest here due to the absolute value function applied to the map.\n",
    "- `sample_rate`: Sample rate for generating the grid in the FFT process (optional, default is `3`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d0496-da6d-49f3-b0bc-1d610ab58205",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Add Phases from Apo Refinement Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce772be-b46a-4d63-9836-1f6b13b41279",
   "metadata": {},
   "source": [
    "The following cells **add phases** to our newly reconstructed datasets. These phases are copied from `refined_path` which were generated via PHENIX in 0.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6af73e-66c7-4f3a-bd1a-0b3cd7758c45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "print(phasing_path)\n",
    "print(vae_reconstructed_with_phases_path)\n",
    "\n",
    "# List of reconstructed mtz files without phases to add phases to\n",
    "file_list = glob.glob(vae_reconstructed_path + \"*.mtz\")\n",
    "# file_list = file_list[:10]\n",
    "file_list_w_phases = file_list.copy()\n",
    "print(\"\\nWorking with \" + str(len(file_list)) + \" MTZs containing reconstructed (apo-like) amplitudes.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86ee2d4-3cab-49b8-b74b-e8511923fdef",
   "metadata": {},
   "source": [
    "#### Linking reconstructed amplitude MTZs to phases MTZs\n",
    "We need to know how to link the MTZ with VAE-reconstructed amplitudes to refinement MTZs containing phases. `valdo.helper.find_phase_file()` can try to do so, but it is best to provide a user-written parser to make sure this happens correctly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e477345-e4f4-4828-9c51-b5be398a6307",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(phasing_path)\n",
    "print(apo_phases_parser(file_list[0]))\n",
    "print(valdo.helper.find_phase_file(file_list[0], phasing_path, parser=apo_phases_parser))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fe3271-91fb-4e79-8f73-aed18bbbf9c3",
   "metadata": {},
   "source": [
    "#### adding phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c46eda-4cdc-41a8-9999-fef2dc9dd207",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if ncpu > 1:\n",
    "    no_phases_files = valdo.helper.add_phases_pool(\n",
    "        file_list, \n",
    "        phasing_path, \n",
    "        vae_reconstructed_with_phases_path, \n",
    "        phase_2FOFC_col_out=phase_2FOFC_col_out, phase_FOFC_col_out=phase_FOFC_col_out,\n",
    "        phase_2FOFC_col_in =phase_2FOFC_col_in,  phase_FOFC_col_in =phase_FOFC_col_in,\n",
    "        prefix=run_prefix, \n",
    "        parser=apo_phases_parser,\n",
    "        ncpu=ncpu)\n",
    "    print(\"Done. No phases found for \" + str(len(no_phases_files)) + \" starting MTZs.\")\n",
    "else:\n",
    "    no_phases_files = valdo.helper.add_phases(\n",
    "        file_list, \n",
    "        phasing_path, \n",
    "        vae_reconstructed_with_phases_path, \n",
    "        phase_2FOFC_col_out=phase_2FOFC_col_out, phase_FOFC_col_out=phase_FOFC_col_out,\n",
    "        phase_2FOFC_col_in =phase_2FOFC_col_in,  phase_FOFC_col_in =phase_FOFC_col_in,\n",
    "        parser=apo_phases_parser\n",
    "    )\n",
    "    print(\"Done. No phases found for \" + str(len(no_phases_files)) + \" starting MTZs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e8b5c2-5ea3-4a03-a3cf-11fa9c5ca216",
   "metadata": {},
   "source": [
    "In the case of dimple refinement for PTP-1B the ones that are missing are the ones for which both indexing solutions gave a poor CC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dcfaa2-bcb3-4e26-9ad8-d05a8855e6fe",
   "metadata": {},
   "source": [
    "##### Add weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade1fba-70f6-48d5-b0f0-5bb62565d020",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "file_list = glob.glob(vae_reconstructed_with_phases_path + \"*.mtz\")\n",
    "print(\"Working with \" + str(len(file_list)) + \" MTZs containing reconstructed (apo-like) amplitudes.\")\n",
    "\n",
    "result = valdo.helper.add_weights(file_list, \n",
    "                                  sigF_col=\"SIGFP\", \n",
    "                                  diff_col=\"diff\",\n",
    "                                  sigdF_pct=w_pcts[0], \n",
    "                                  absdF_pct=w_pcts[1],\n",
    "                                  redo=True,\n",
    "                                  low_res_cutoff=low_res_cutoff,\n",
    "                                  ncpu=ncpu)\n",
    "print(\"Added weights and weighted differences to \" + str(sum(result)) + \" MTZ files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ad979b-a83e-457b-a4ef-323c385e523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "file_list = glob.glob(vae_reconstructed_with_phases_path + \"*.mtz\")\n",
    "result=valdo.helper.extrapolate(file_list, \\\n",
    "                                F_col=amplitude_scaled_col, \\\n",
    "                                sigF_col=error_scaled_col, \\\n",
    "                                recons_col=\"recons\", \\\n",
    "                                sigF_recons_col=\"SIG_recons\", \\\n",
    "                                diff_col=\"diff\", \\\n",
    "                                wt_col=\"WT\", \\\n",
    "                                redo=True, \\\n",
    "                                weighted=False, \\\n",
    "                                extrapolate_factors=[4,8,16], \\\n",
    "                                ncpu=ncpu)\n",
    "print(\"Calculated extrapolated structure factor amplitudes for \" + str(sum(result)) + \" MTZ files.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5dfc24-edc3-4930-89c7-0ed1aadc3087",
   "metadata": {},
   "source": [
    "### Gaussian Blurring Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eba489-51a5-43c8-ac68-6c7efb4125fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "The following two cells complete gaussian blurring and blob searching. For the blurring, by default, he radius is set to `5A` with `sigma = 5/3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840accda-3fda-4f7e-8d43-40a13193c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of reconstructed mtz files (with phases) to identify blobs in\n",
    "# tmp=\"/n/holyscratch01/hekstra_lab/dhekstra/valdo-tests/pipeline_run2/vae/reconstructed_w_phases/\"\n",
    "\n",
    "# usually vae_reconstructed_with_phases_path rather than tmp\n",
    "file_list = glob.glob(vae_reconstructed_with_phases_path + \"*mtz\")\n",
    "print(\"Retrieving \" + str(len(file_list)) + \" files for blob analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d004e3b-88c9-41ec-b3cf-20847dff62ce",
   "metadata": {},
   "source": [
    "The following takes about 0.3 s/dataset on a single CPU for PTP1B and will scale with the number of voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b972c767-ac5e-460c-a032-a26d48fe1299",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Function in valdo.blobs that generates a list of blobs\n",
    "\n",
    "if ncpu>1:\n",
    "    valdo.blobs.generate_blobs_pool(\n",
    "        input_files=file_list, \n",
    "        model_folder=refined_path, \n",
    "        diff_col='WDF', # DEFAULT is \"diff\"!!!!\n",
    "        phase_col='refine_PH2FOFCWT', \n",
    "        output_folder=blob_path, \n",
    "        prefix=run_prefix,\n",
    "        cutoff=blob_sig_cutoff,\n",
    "        radius_in_A=radius_in_A,\n",
    "        ncpu=ncpu)\n",
    "else:\n",
    "    valdo.blobs.generate_blobs(\n",
    "        input_files=file_list, \n",
    "        model_folder=refined_path, \n",
    "        diff_col='WDF', \n",
    "        phase_col='refine_PH2FOFCWT', \n",
    "        output_folder=blob_path, \n",
    "        prefix=run_prefix,\n",
    "        cutoff=blob_sig_cutoff,\n",
    "        radius_in_A=radius_in_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a148fd9d-8164-48bb-a703-7da86dde87c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 9: Identifying Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b7ce19-3844-41c8-b1c9-0c7a944b0e63",
   "metadata": {},
   "source": [
    "In this final step, the highest scoring blobs returned in the previous step can be analyzed individually. If the blob is plausibly a ligand, refinement with a ligand may be completed to determine whether or not the blob can be considered a \"hit.\"\n",
    "\n",
    "Blobs that are returned can be related to various other events, not just ligand binding. Examples may include ligand-induced conformational change (which would still indicate the presence of a ligand) or various other unrelated conformational changes, such as radiation damage or, for example for PTP1B, cysteine oxidation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2565c239-9c93-430c-a549-78ca304851e8",
   "metadata": {},
   "source": [
    "In the following example, we have also included the evaluation of our method, via AUC, in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2323e869-9ec5-48f3-86f6-2f0985eaa748",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Tagging Blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d0a6a4-cc2a-4040-ad32-ba7c8f804f94",
   "metadata": {},
   "source": [
    "In this section, we tag and filter the blobs. We remove...\n",
    "\n",
    "- blobs that are duplicates (we occassionally have duplicate blobs due to an issue with gemmi's ASU mask)\n",
    "- blobs associated with the oxidation of `cys215`\n",
    "- blobs that belong to low quality samples (high r factors in refinement)\n",
    "- blobs that belong to samples with inconsistent data (in particular, Helen Ginn lists a few samples as hits without including a ligand in their bound state).\n",
    "\n",
    "We also label blobs that are close to ligand positions known from a previous PanDDA analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50b2d69-ab5f-4c68-a953-86f74ba1ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_df = pd.read_pickle(blob_path + run_prefix + 'blob_stats.pkl')\n",
    "blob_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f79b39-9398-4235-aea6-2eb06b759527",
   "metadata": {
    "tags": []
   },
   "source": [
    "Tag Samples that are known to contain a ligand (``Bound''): (1 if bound, 0 otherwise). Based on a previous PanDDA analysis. You can skip this if no such analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3488259-cb23-4d7a-8bf4-d10f8da48c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_df[\"bound\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc644be0-423e-4514-a8ed-c449bd24812f",
   "metadata": {},
   "source": [
    "In the case of PTP-1B, the catalytic cysteine, residue 215, is prone to oxidation with levels varying among datasets. To get rid of these false positives, we tag Samples within 3A of a Cys215 Atom (1 if within, 0 otherwise). You can use/adapt `valdo.tag.tag_blobs_around_seqid` more generally to omit certain regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bfa05c-337c-4aa6-a46a-a0b26c23ab8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#with one CPU, this takes about 40 sec\n",
    "\n",
    "# blob_df = valdo.tag.tag_blobs_around_seqid(blob_df, \n",
    "#                                            refined_path, \n",
    "#                                            radius=5, # increased from 3\n",
    "#                                            tag='cys215', \n",
    "#                                            focal_seqid=215, \n",
    "#                                            ncpu=ncpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41e2def-1c2b-4395-b448-3a2b12be3bd9",
   "metadata": {},
   "source": [
    "In the case of PTP-1B, we already had an analysis by Keedy et al in hand using PanDDA. Here we tag samples within `r` of a known LIG atom (1 if yes, 0 otherwise). Fortunately, in all their bound models, the ligand \"residue name\" was LIG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61046323-1918-4491-ad08-8b38274dab46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# blob_df = valdo.tag.tag_lig_blobs(blob_df, \n",
    "#                                   bound_models_standardized_path,\n",
    "#                                   ncpu=ncpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf32b713-d46b-48d8-91b9-621c2f974d76",
   "metadata": {},
   "source": [
    "We use GEMMI under the hood to restrict blobs to a single asymmetric unit. Sometimes, however, we still double-include blobs if they are not in the exact same position. Here we tag Blobs that are Duplicates of Other Blobs (a.k.a. Patch for Gemmi's ASU Mask Issue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85978d50-d318-4314-811b-214c1c3c54fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Identifies all possible cartesian coordinates after symmetry operations\n",
    "# blob_df[['fractional', 'all_possible_frac', 'all_possible_cart']] = blob_df.apply(determine_locations, args=(vae_reconstructed_with_phases_path,), axis=1)\n",
    "blob_df = valdo.tag.determine_locations(blob_df, vae_reconstructed_with_phases_path, ncpu=ncpu)\n",
    "# blob_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc0a7b0-1e17-4926-8ca8-24bb0dc6f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# <1 sec\n",
    "# Marks blobs as duplicates if they are within 1A of another blob in the same sample\n",
    "blob_df = valdo.tag.mark_duplicates(blob_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9f581c-6283-4705-87ca-bab26ed539da",
   "metadata": {},
   "source": [
    "Let's look at some summary statistics so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73633b48-d768-4ba7-ab87-44e1af19a6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387ec19d-fd35-4c86-8dd7-2e13b9d6050f",
   "metadata": {},
   "source": [
    "Tag Blobs Belonging to Samples with Low cc after scalings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c165e2-1a22-4560-940f-3bd596af5ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case, no blobs are removed because we already did so before\n",
    "metrics_df = pd.read_pickle(scaled_path + run_prefix + \"scaling_metrics.pkl\")\n",
    "\n",
    "low_end_corr = metrics_df.loc[metrics_df['end_corr'] < min_cc_ref, 'file'].tolist()\n",
    "blob_df['low_end_corr'] = blob_df['sample'].isin(low_end_corr)\n",
    "print(\"Number of blobs with end_corr < \" + str(min_cc_ref) + \" = \" +str(blob_df['low_end_corr'].sum()))\n",
    "\n",
    "# Should be 0 since we did this already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c497905-2710-41ff-9262-371fa5847266",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_df.to_pickle(blob_path + run_prefix + 'blob_stats_tagged.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bf44bc-bb63-49d8-9e0e-21e9bf1d055f",
   "metadata": {},
   "source": [
    "### Filter Blobs \n",
    "Remove blobs near Cys215, with high R_free, duplicates, and more as specified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1dc04-ae37-43bd-a002-2eeeba6d225f",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_df = pd.read_pickle(blob_path + run_prefix +'blob_stats_tagged.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b794a6d-7781-493a-be20-0eed978338c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_df['sample_id'] = blob_df[\"sample\"].apply(lambda x: x.split(\"_\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54f2246-d2ac-408e-aab1-aed7e9d03c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Cys215 related blobs, and duplicates\n",
    "\n",
    "blob_df = blob_df[(blob_df['duplicate']==0)]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2debc4-96cb-477a-88eb-127b36cd8184",
   "metadata": {},
   "source": [
    "Let's look at the filtered set of blobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca34f668-57cc-43e9-a3ca-b5751036f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8741a6e-8ee9-45ba-bac7-2fc55686be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_df.to_pickle(blob_path + run_prefix+'filtered_blob_stats_tagged.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ca000-9f0b-477d-8ab0-ddfffce22e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_blobs = blob_df.sort_values(by=\"peakz\",axis=0,ascending=False)\n",
    "sorted_blobs.iloc[0:40,:][[\"sample\",\"peakz\",\"score\",\"bound\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944cdbe7-182b-45cf-8b37-a5901263bfac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d435a0-1a0d-4dfc-8e4b-14d4032b6cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c9b756-d8c1-4804-bb00-2984c893ae94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
